{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Sensor Bias\n",
    "\n",
    "We want to compute the joint posterior over sensors' biases in a 2-D tracking setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import funsor\n",
    "import funsor.pyro\n",
    "import funsor.distributions as f_dist\n",
    "import funsor.ops as ops\n",
    "from funsor.pyro.convert import dist_to_funsor, mvn_to_funsor, matrix_and_mvn_to_funsor, tensor_to_funsor\n",
    "from funsor.interpreter import interpretation, reinterpret\n",
    "from funsor.optimizer import apply_optimizer\n",
    "from funsor.terms import lazy\n",
    "from funsor.domains import bint, reals\n",
    "from funsor.torch import Tensor, Variable\n",
    "from funsor.sum_product import sequential_sum_product\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sensors = 5\n",
    "num_frames = 100\n",
    "\n",
    "# simulate biased sensors\n",
    "sensors  = []\n",
    "for _ in range(num_sensors):\n",
    "    bias = 0.5 * torch.randn(2)\n",
    "    sensors.append(bias)\n",
    "\n",
    "# simulate a single track\n",
    "track = []\n",
    "z = 10 * torch.rand(2)  # initial state\n",
    "v = 2 * torch.randn(2)  # velocity\n",
    "for t in range(num_frames):\n",
    "    # Advance latent state.\n",
    "    z += v + 0.1 * torch.randn(2)\n",
    "#     z.clamp_(min=0, max=10)  # keep in the box\n",
    "    \n",
    "    # Observe via a random sensor.\n",
    "    sensor_id = pyro.sample('id', dist.Categorical(torch.ones(num_sensors)))\n",
    "    x = z - sensors[sensor_id]\n",
    "    track.append({\"sensor_id\": sensor_id, \"x\": x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a tracking problem in Funsor. We start by modeling the biases of each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO transform this to cholesky decomposition\n",
    "# print(bias_cov.shape)\n",
    "# bias_cov = bias_cov @ bias_cov.t()\n",
    "# create a joint Gaussian over biases\n",
    "\n",
    "covs = [torch.eye(2, requires_grad=True) for i in range(num_sensors)]\n",
    "bias_dist = 0.\n",
    "for i in range(num_sensors):\n",
    "    bias += funsor.pyro.convert.mvn_to_funsor(\n",
    "        dist.MultivariateNormal(torch.zeros(2), covs[i]),\n",
    "#         event_dims=(\"pos\",),\n",
    "#         real_inputs=OrderedDict([(\"bias_{}\".format(i), reals(2))])\n",
    "        real_inputs=OrderedDict([(\"bias\", reals(2))])\n",
    "    )(value=\"bias_{}\".format(i))\n",
    "bias_dist.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "bias_scale = torch.ones(2, requires_grad=True)  # This can be learned\n",
    "bias_dist = funsor.pyro.convert.mvn_to_funsor(\n",
    "    dist.MultivariateNormal(\n",
    "        torch.zeros(num_sensors * 2),\n",
    "        bias_scale_tril.expand(num_sensors, 2).reshape(-1).diag_embed()\n",
    "    )\n",
    "    OrderedDict(bias=reals(num_sensors, 2))\n",
    ")\n",
    "bias_dist.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the filter in funsor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# this can be parameterized by a lower dimensional vector \n",
    "# to learn a structured transition matrix\n",
    "# eg a GP with a matern v=3/2 kernel\n",
    "# see paper for details \n",
    "transition_matrix = torch.randn(2, 2, requires_grad=True)\n",
    "\n",
    "def model(track):\n",
    "    init_dist = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "    transition_dist = torch.distributions.MultivariateNormal(\n",
    "        torch.zeros(2),\n",
    "        torch.eye(2))\n",
    "    observation_matrix = torch.eye(2) + 0.2 * torch.randn(2, 2)\n",
    "    observation_dist = torch.distributions.MultivariateNormal(\n",
    "        torch.zeros(2),\n",
    "        torch.eye(2))\n",
    "\n",
    "    init = dist_to_funsor(init_dist)(value=\"state\")\n",
    "    # inputs are the previous state ``state`` and the next state\n",
    "    trans = matrix_and_mvn_to_funsor(transition_matrix, transition_dist,\n",
    "                                     (\"time\",), \"state\", \"state(time=1)\")\n",
    "    obs = matrix_and_mvn_to_funsor(observation_matrix, observation_dist,\n",
    "                                   (\"time\",), \"state(time=1)\", \"value\")\n",
    "    \n",
    "    # Now this is the crux, we add bias to the observation\n",
    "    sensor_ids = Tensor(\n",
    "        torch.tensor([frame[\"sensor_id\"] for frame in track]),\n",
    "        OrderedDict([(\"time\", bint(num_frames))]),\n",
    "        dtype=len(sensors)\n",
    "    )\n",
    "    biased_observations = Tensor(\n",
    "        torch.stack([frame[\"x\"] for frame in track]),\n",
    "        OrderedDict([(\"time\", bint(num_frames))])\n",
    "    )\n",
    "    \n",
    "    # incorporate sensor id in the observation\n",
    "#     bias_over_time = bias(value=sensor_ids)\n",
    "    # bias_over_time = bias(bias=biased_observations)\n",
    "    # inputs: bias shape (num_sensors, 2), sensor_ids\n",
    "    # outputs: 2\n",
    "    bias = Variable(\"bias\", reals(num_sensors, 2))[sensor_ids]\n",
    "    \n",
    "    debiased_observations = biased_observations - bias\n",
    "    obs = obs(value=debiased_observations)\n",
    "    print(obs)\n",
    "    \n",
    "    # Similar to funsor.pyro.hmm.GaussianHMM.log_prob()\n",
    "    # ndims = max(len(batch_shape), value.dim() - event_dim)\n",
    "    # value = tensor_to_funsor(value, (\"time\",), event_output=event_dim - 1,\n",
    "    #                          dtype=self.dtype)\n",
    "\n",
    "    # obs = obs(value=value)\n",
    "    logp = trans + obs + bias_dist\n",
    "\n",
    "    # collapse out the time variable\n",
    "    logp = sequential_sum_product(ops.logaddexp, ops.add,\n",
    "                                  logp, \"time\", {\"state\": \"state(time=1)\"})\n",
    "    logp += init\n",
    "    # logaddexp across all states\n",
    "    logp = logp.reduce(ops.logaddexp, frozenset([\"state\", \"state(time=1)\"]))\n",
    "#     # ensure we collapsed out the right dim\n",
    "#     assert logp.data.dim() == 0\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Finally we have a result that is a joint Gaussian over the biases.\n",
    "We can\n",
    "1. optimize all parameters to maximize `result`\n",
    "2. estimate the joint distribution over all bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "params = covs.copy()\n",
    "params.append(transition_matrix)\n",
    "optim = Adam(params, lr=1e-3)\n",
    "for i in range(num_epochs):\n",
    "    optim.zero_grad()\n",
    "    with interpretation(lazy):\n",
    "        log_prob = apply_optimizer(model(track))\n",
    "    loss = -reinterpret(log_prob).data\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print(loss)\n",
    "    optim.step()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the joint posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
