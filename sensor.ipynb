{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Sensor Bias\n",
    "\n",
    "We want to compute the joint posterior over sensors' biases in a 2-D tracking setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import funsor\n",
    "import funsor.pyro\n",
    "import funsor.distributions as f_dist\n",
    "import funsor.ops as ops\n",
    "from funsor.pyro.convert import dist_to_funsor, mvn_to_funsor, matrix_and_mvn_to_funsor, tensor_to_funsor\n",
    "from funsor.interpreter import interpretation, reinterpret\n",
    "from funsor.optimizer import apply_optimizer\n",
    "from funsor.terms import lazy\n",
    "from funsor.domains import bint, reals\n",
    "from funsor.sum_product import sequential_sum_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sensors = 5\n",
    "num_frames = 100\n",
    "\n",
    "# simulate biased sensors\n",
    "sensors  = []\n",
    "for _ in range(num_sensors):\n",
    "    true_pos = 10 * torch.rand(2)  # in a box\n",
    "    biased_pos = true_pos + torch.randn(2)\n",
    "    sensors.append({\n",
    "        \"true_pos\": true_pos,\n",
    "        \"biased_pos\": biased_pos,\n",
    "    })\n",
    "\n",
    "# simulate a single track\n",
    "track = []\n",
    "z = 10 * torch.rand(2)  # initial state\n",
    "for t in range(num_frames):\n",
    "    # Advance latent state.\n",
    "    z += torch.randn(2)\n",
    "    z.clamp_(min=0, max=10)  # keep in the box\n",
    "    \n",
    "    # Observe via a random sensor.\n",
    "    sensor_id = pyro.sample('id', dist.Categorical(torch.ones(num_sensors)))\n",
    "    x = z - sensors[sensor_id][\"biased_pos\"]\n",
    "    track.append({\"sensor_id\": sensor_id, \"x\": x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a tracking problem in Funsor. We start by modeling the biases of each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bias = funsor.pyro.convert.mvn_to_funsor(\n",
    "#     dist.MultivariateNormal(\n",
    "#         torch.zeros(num_sensors, 2),\n",
    "#         torch.eye(2, requires_grad=True)  # This can be learned\n",
    "#     ),\n",
    "#     event_dims=(\"biased_pos\",),\n",
    "#     real_inputs=OrderedDict([(\"biased_pos\", reals(2))])\n",
    "# )(value=\"bias\")\n",
    "\n",
    "bias_cov = torch.eye(2, requires_grad=True)  # This can be learned\n",
    "# Instead create a giant joint Gaussian:\n",
    "bias = sum(\n",
    "    funsor.pyro.convert.mvn_to_funsor(\n",
    "        dist.MultivariateNormal(\n",
    "            torch.zeros(2),\n",
    "            cov\n",
    "        ),\n",
    "        event_dims=((\"bias_{}\".format(j) for j in range(num_sensors)),)\n",
    "    )(value=\"bias_{}\".format(i))\n",
    "    for i in range(num_sensors)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the filter in funsor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(track):\n",
    "    # Similar to funsor.pyro.hmm.GaussianHMM.__init__()\n",
    "    init_dist = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "    # this can be parameterized by a lower dimensional vector \n",
    "    # to learn a structured transition matrix\n",
    "    # eg a GP with a matern v=3/2 kernel\n",
    "    transition_matrix = torch.randn(2, 2, requires_grad=True)\n",
    "\n",
    "    transition_dist = torch.distributions.MultivariateNormal(\n",
    "        torch.zeros(2),\n",
    "        torch.eye(2))\n",
    "    observation_matrix = torch.randn(2, 2)\n",
    "    observation_dist = torch.distributions.MultivariateNormal(\n",
    "        torch.zeros(2),\n",
    "        torch.eye(2))\n",
    "\n",
    "    init = dist_to_funsor(init_dist)(value=\"state\")\n",
    "    trans = matrix_and_mvn_to_funsor(transition_matrix, transition_dist,\n",
    "                                     (\"time\",), \"state\", \"state(time=1)\")\n",
    "    obs = matrix_and_mvn_to_funsor(observation_matrix, observation_dist,\n",
    "                                   (\"time\",), \"state(time=1)\", \"value\")\n",
    "    # Now this is the crux, we add bias to the observation\n",
    "    sensor_ids = funsor.torch.Tensor(\n",
    "        torch.tensor([frame[\"sensor_id\"] for frame in track]),\n",
    "        OrderedDict([(\"sensor_id\", bint(num_frames))]),\n",
    "        dtype=len(sensors)\n",
    "    )\n",
    "    biased_observations = funsor.torch.Tensor(\n",
    "        torch.stack([frame[\"x\"] for frame in track]),\n",
    "        OrderedDict([(\"sensor_id\", bint(num_frames))])\n",
    "    )\n",
    "    bias_over_time = bias(sensor_id=sensor_ids)\n",
    "    obs = obs(value=biased_observations)\n",
    "\n",
    "    # Similar to funsor.pyro.hmm.GaussianHMM.log_prob()\n",
    "    # ndims = max(len(batch_shape), value.dim() - event_dim)\n",
    "    # value = tensor_to_funsor(value, (\"time\",), event_output=event_dim - 1,\n",
    "    #                          dtype=self.dtype)\n",
    "\n",
    "    # obs = obs(value=value)\n",
    "    result = trans + obs\n",
    "\n",
    "    result = sequential_sum_product(ops.logaddexp, ops.add,\n",
    "                                    result, \"time\", {\"state\": \"state(time=1)\"})\n",
    "    result += init\n",
    "    result = result.reduce(ops.logaddexp, frozenset([\"state\", \"state(time=1)\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Finally we have a result that is a joint Gaussian over the biases.\n",
    "We can\n",
    "1. optimize all parameters to maximize `result.reduce(obs.logaddexp)`\n",
    "2. estimate the joint distribution over all bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "params = [bias_cov, transition_matrix]\n",
    "optim = Adam(params, lr=1e-3)\n",
    "for i in range(num_epochs):\n",
    "    optim.zero_grad()\n",
    "    with interpretation(lazy):\n",
    "        log_prob = apply_optimizer(model(track))\n",
    "    loss = -reinterpret(log_prob).data\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print(loss)\n",
    "    optim.step()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the joint posterior distribution."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
