{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: FUNSOR_TYPECHECK=1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import functools\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "%env FUNSOR_TYPECHECK=1\n",
    "import funsor\n",
    "from funsor.terms import Funsor, Variable, Number, Lambda, Slice\n",
    "from funsor.tensor import Tensor\n",
    "from funsor.domains import Array, Bint, Real, Reals\n",
    "from funsor.factory import Bound, Fresh, Has, Value, make_funsor, to_funsor\n",
    "import funsor.ops as ops\n",
    "from funsor.cnf import Contraction\n",
    "from funsor.testing import random_tensor\n",
    "from funsor.interpretations import reflect, memoize\n",
    "import funsor.torch.distributions as dist\n",
    "\n",
    "funsor.set_backend(\"torch\")\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  X^0 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}} \\\\\n",
    "  X^1 &= \\sigma(W^1 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}input}}}{\\vphantom{fg}\\odot}} X^0 + b^1) & W^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1 \\times \\mathsf{\\vphantom{fg}input}} & b^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1} \\\\\n",
    "  X^2 &= \\sigma(W^2 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_1}}{\\vphantom{fg}\\odot}} X^1 + b^2) & W^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2 \\times \\mathsf{\\vphantom{fg}hidden}_1} & b^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2} \\\\\n",
    "  X^3 &= \\sigma(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_2}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}\\times \\mathsf{\\vphantom{fg}hidden}_2} & b^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "X0 = random_tensor(\n",
    "    OrderedDict([(\"input_layer\", Bint[input_dim])])\n",
    ")\n",
    "\n",
    "hidden_1_dim = 32\n",
    "W1 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"input_layer\", Bint[input_dim]),\n",
    "        (\"hidden_layer_1\", Bint[hidden_1_dim])\n",
    "    ])\n",
    ")\n",
    "b1 = random_tensor(\n",
    "    OrderedDict([(\"hidden_layer_1\", Bint[hidden_1_dim])])\n",
    ")\n",
    "X1 = ((W1 * X0).reduce(ops.add, \"input_layer\") + b1).sigmoid()\n",
    "\n",
    "hidden_2_dim = 16\n",
    "W2 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden_layer_1\", Bint[hidden_1_dim]),\n",
    "        (\"hidden_layer_2\", Bint[hidden_2_dim])\n",
    "    ])\n",
    ")\n",
    "b2 = random_tensor(\n",
    "    OrderedDict([(\"hidden_layer_2\", Bint[hidden_2_dim])])\n",
    ")\n",
    "X2 = ((W2 * X1).reduce(ops.add, \"hidden_layer_1\") + b2).sigmoid()\n",
    "\n",
    "hidden_3_dim = 8\n",
    "W3 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden_layer_2\", Bint[hidden_2_dim]),\n",
    "        (\"hidden_layer_3\", Bint[hidden_3_dim])\n",
    "    ])\n",
    ")\n",
    "b3 = random_tensor(\n",
    "    OrderedDict([(\"hidden_layer_3\", Bint[hidden_3_dim])])\n",
    ")\n",
    "X3 = ((W3 * X2).reduce(ops.add, \"hidden_layer_2\") + b3).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def FullConnLayer(\n",
    "    x: Has[{\"layer\"}],\n",
    "    W: Has[{\"layer\"}],\n",
    "    b: Funsor,\n",
    "    layer: Bound\n",
    ") -> Fresh[lambda x: x]:\n",
    "    result = ((W * x).reduce(ops.add, layer) + b).sigmoid()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([6.7379e-03, 6.5177e-02, 9.8971e-01, 9.9932e-01, 1.0000e+00, 9.1053e-01,\n",
       "        1.0000e+00, 2.1048e-03, 1.2270e-02, 5.3009e-01, 9.9986e-01, 1.8652e-02,\n",
       "        2.8300e-01, 9.9843e-01, 9.9932e-01, 9.9806e-01, 9.4389e-01, 9.9555e-01,\n",
       "        9.9038e-01, 2.9201e-05, 1.9454e-01, 7.2684e-01, 9.9984e-01, 1.0000e+00,\n",
       "        1.4294e-01, 3.8029e-04, 3.7336e-03, 4.4616e-05, 1.0000e+00, 9.9998e-01,\n",
       "        4.2059e-01, 2.0911e-10]), OrderedDict([('out_layer', Bint[32, ])]), 'real')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 100\n",
    "X0 = random_tensor(\n",
    "    OrderedDict([(\"layer\", Bint[input_dim])])\n",
    ")\n",
    "hidden_1_dim = 32\n",
    "W1 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"layer\", Bint[input_dim]),\n",
    "        (\"out_layer\", Bint[hidden_1_dim])\n",
    "    ])\n",
    ")\n",
    "b1 = random_tensor(\n",
    "    OrderedDict([(\"out_layer\", Bint[hidden_1_dim])])\n",
    ")\n",
    "\n",
    "X1 = FullConnLayer(X0, W1, b1, \"layer\")\n",
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "@make_funsor\n",
    "def RecurrentLayer(\n",
    "    x: Funsor,\n",
    "    Wh: Funsor,\n",
    "    Wi: Funsor,\n",
    "    b: Funsor,\n",
    "    hidden: Bound,\n",
    "    input: Bound\n",
    ") -> Fresh[lambda x: x]:\n",
    "    output = ((Wh * h).reduce(ops.add, \"hidden\") + (Wi * x).reduce(ops.add, \"input\") + b).sigmoid()\n",
    "    return output(hidden=\"new_hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Softmax(\n",
    "    x: Funsor,\n",
    "    ax: Bound,\n",
    "    ax2: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    x = x(**{ax.name: ax2.name})\n",
    "    y = x - x.reduce(ops.logaddexp, ax2)\n",
    "    return y.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\text{Attention} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}val}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}val}} \\\\\n",
    "\\text{Attention}(Q, K, V, M) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\mathrm{softmax}}} \\left( \\frac{Q \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}key}}}{\\vphantom{fg}\\odot}} K}{\\sqrt{|\\mathsf{\\vphantom{fg}key}|}} + M \\right) \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\odot}} V.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Attention(\n",
    "    Q: Has[{\"key\"}],\n",
    "    K: Has[{\"key\", \"seq\"}],\n",
    "    V: Has[{\"seq2\"}],\n",
    "    M: Has[{\"seq\"}],\n",
    "    key: Bound,\n",
    "    seq: Bound,\n",
    "    seq2: Bound\n",
    ") -> Fresh[lambda Q: Q]:\n",
    "    x = (Q * K).reduce(ops.add, key) / math.sqrt(key.output.size) + M\n",
    "    return (Softmax(x, seq, seq2) * V).reduce(ops.add, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([ 0.5775, -0.4559,  0.4790,  0.6897,  0.2296]), OrderedDict([('val', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = random_tensor(OrderedDict([(\"key\", Bint[10])]))\n",
    "k = random_tensor(OrderedDict([(\"key\", Bint[10]), (\"seq\", Bint[3])]))\n",
    "v = random_tensor(OrderedDict([(\"seq2\", Bint[3]), (\"val\", Bint[5])]))\n",
    "m = random_tensor(OrderedDict([(\"seq\", Bint[3])]))\n",
    "Attention(q, k, v, m, \"key\", \"seq\", \"seq2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n-|\\mathsf{\\vphantom{fg}kernel}|+1], \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X &= Y,\\ \\text{where} \\\\\n",
    "  Y_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}kernel}(j)} &= X_{\\mathsf{\\vphantom{fg}seq}(i+j - 1)}.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Unroll(\n",
    "    x: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size - k + 1]]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    return x(**{seq.name: seq2 + kernel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\text{Conv1d} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n^\\prime]} \\\\\n",
    "\\text{Conv1d}(X; W, b) &= W \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\odot}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X + b\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "W &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "b &\\in \\mathbb{R}\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Conv1d(\n",
    "    X: Has[{\"chans\", \"seq\"}],\n",
    "    W: Has[{\"chans\", \"kernel\"}],\n",
    "    b: Funsor,\n",
    "    chans: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Bound,\n",
    "    seq: Bound,\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size - k + 1]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = W * Unroll(X, seq, k, kernel, seq2)\n",
    "    return y.reduce(ops.add, frozenset({chans, kernel})) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"seq\", Bint[10])]))\n",
    "kernel = Variable(\"kernel\", Bint[3])\n",
    "w = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"kernel\", Bint[3])]))\n",
    "b = random_tensor(OrderedDict([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([ 2.3575,  0.3245, -3.2575,  0.5397,  1.4327, -1.0565, -1.4984,  1.0468]), OrderedDict([('seq2', Bint[8, ])]), 'real')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv1d(x, w, b, \"chans\", 3, \"kernel\", \"seq\", \"seq2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Conv2d(\n",
    "    X: Has[{\"chans\", \"height\", \"width\"}],\n",
    "    W: Has[{\"chans\", \"kh\", \"kw\"}],\n",
    "    b: Funsor,\n",
    "    chans: Bound,\n",
    "    kh_size: Value[int],\n",
    "    kh: Bound,\n",
    "    height: Bound,\n",
    "    height2: Fresh[lambda height, kh_size: Bint[height.size - kh_size + 1]],\n",
    "    kw_size: Value[int],\n",
    "    kw: Bound,\n",
    "    width: Bound,\n",
    "    width2: Fresh[lambda width, kw_size: Bint[width.size - kw_size + 1]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = W * Unroll(Unroll(X, width, kw_size, kw, width2), height, kh_size, kh, height2)\n",
    "    return y.reduce(ops.add, frozenset({chans, kh, kw})) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[-6.5971,  8.6155,  4.7653,  4.5988,  3.9547],\n",
       "        [ 3.2545,  5.7080,  2.5373,  0.5605, 10.9278],\n",
       "        [ 5.1632,  8.2678,  7.8185,  3.3192, -2.3040],\n",
       "        [-1.8145,  2.7405, -8.2361,  3.2329, 13.0263],\n",
       "        [-3.6424, 16.5613,  4.2129,  5.0539,  0.5886],\n",
       "        [ 1.2888,  4.2768, -1.1993, -1.6411,  8.0008],\n",
       "        [ 0.6596, 12.1573, -3.1636,  2.7850,  2.2961],\n",
       "        [ 5.0141,  5.8857,  8.3560,  0.7796, 14.1677]]), OrderedDict([('height2', Bint[8, ]), ('width2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"height\", Bint[10]), (\"width\", Bint[8])]))\n",
    "w = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"kh\", Bint[3]), (\"kw\", Bint[4])]))\n",
    "b = random_tensor(OrderedDict([]))\n",
    "\n",
    "Conv2d(x, w, b, \"chans\", 3, \"kh\", \"height\", \"height2\", 4, \"kw\", \"width\", \"width2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Pool(\n",
    "    x: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]], # seq -> Bint[]\n",
    ") -> Fresh[lambda x: x]: # x -> x.output (Bint[] or Real)\n",
    "    assert not seq.output.size % k\n",
    "    return x(**{seq.name: seq2 * Number(k, k+1) + kernel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[-0.5920, -1.4516],\n",
       "        [-1.5946, -0.0205],\n",
       "        [-1.4622, -0.9688],\n",
       "        [ 0.2946,  0.4871],\n",
       "        [-1.1216, -0.0153]]), OrderedDict([('seq2', Bint[5, ]), ('kernel', Bint[2, ])]), 'real')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = Pool(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool1d(\n",
    "    X: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Pool(X, seq, k, kernel, seq2).reduce(ops.max, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([-0.1638,  0.8504,  0.7523, -1.0736,  0.0528]), OrderedDict([('seq2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = MaxPool1d(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool2d(\n",
    "    X: Has[{\"height\", \"width\"}],\n",
    "    height: Bound,\n",
    "    kh_size: Value[int],\n",
    "    kh: Fresh[lambda kh_size: Bint[kh_size]],\n",
    "    height2: Fresh[lambda height, kh_size: Bint[height.size // kh_size]],\n",
    "    width: Bound,\n",
    "    kw_size: Value[int],\n",
    "    kw: Fresh[lambda kw_size: Bint[kw_size]],\n",
    "    width2: Fresh[lambda width, kw_size: Bint[width.size // kw_size]],\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = Pool(Pool(X, height, kh_size, kh, height2), width, kw_size, kw, width2)\n",
    "    return y.reduce(ops.max, frozenset({kh, kw}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[0.6025, 1.3002],\n",
       "        [0.4977, 1.1604],\n",
       "        [0.7279, 0.8614]]), OrderedDict([('width2', Bint[3, ]), ('height2', Bint[2, ])]), 'real')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"width\", Bint[9]), (\"height\", Bint[4])]))\n",
    "Y = MaxPool2d(X, \"height\", 2, \"kh\", \"height2\", \"width\", 3, \"kw\", \"width2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Pool2(\n",
    "    x: Funsor,\n",
    "    seq: Bound,\n",
    "    kernel: Funsor,\n",
    "    seq2: Fresh[lambda seq, kernel: Bint[seq.size // kernel.size]], # seq -> Bint[]\n",
    ") -> Fresh[lambda x: x]: # x -> x.output (Bint[] or Real)\n",
    "    return x(**{seq.name: seq2 * Number(kernel.output.size, kernel.output.size+1) + kernel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "@make_funsor\n",
    "def Mean(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.mean(funsor.terms.Lambda(ax, X), 0)\n",
    "\n",
    "@make_funsor\n",
    "def Variance(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.var(funsor.terms.Lambda(ax, X), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "@make_funsor\n",
    "def Mean(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, ax) / ax.output.size\n",
    "\n",
    "@make_funsor\n",
    "def Mean2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, frozenset({ax, ax2})) / (ax.output.size * ax2.output.size)\n",
    "\n",
    "@make_funsor\n",
    "def Variance(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean((X - Mean(X, ax))**2, ax)\n",
    "\n",
    "\n",
    "@make_funsor\n",
    "def Variance2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean2((X - Mean2(X, ax, ax2))**2, ax, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Standardize(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax})\n",
    "    return (y - Mean(X, ax)) / (Variance(X, ax) + ops.finfo(X.data).eps).sqrt()\n",
    "\n",
    "@make_funsor\n",
    "def Standardize2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax],\n",
    "    new_ax2: Fresh[lambda ax2: ax2]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax, ax2.name: new_ax2})\n",
    "    return (y - Mean2(X, ax, ax2)) / (Variance2(X, ax, ax2) + ops.finfo(X.data).eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def BatchNorm(\n",
    "    X: Has[{\"batch\", \"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    batch: Bound,\n",
    "    layer: Bound,\n",
    "    batch2: Fresh[lambda batch: batch],\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize2(X, batch, layer, batch2, layer2) * gamma + beta\n",
    "\n",
    "@make_funsor\n",
    "def InstanceNorm(\n",
    "    X: Has[{\"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    layer: Bound,\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize(X, layer, layer2) * gamma + beta\n",
    "\n",
    "# same as BatchNorm\n",
    "@make_funsor\n",
    "def LayerNorm(\n",
    "    X: Has[{\"chans\", \"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    chans: Bound,\n",
    "    layer: Bound,\n",
    "    chans2: Fresh[lambda chans: chans],\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize2(X, chans, layer, chans2, layer2) * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[-0.5280,  1.0413, -2.6600, -0.4790, -0.6751],\n",
       "         [-0.7684, -0.0121, -1.3248, -0.7829, -0.2051],\n",
       "         [-3.5092, -0.9516, -0.2821, -0.7431, -1.6872]],\n",
       "\n",
       "        [[-1.4081, -0.9501, -3.5667,  0.4102, -1.9272],\n",
       "         [-0.4956, -0.5375, -0.5311, -0.0411, -0.7753],\n",
       "         [ 1.2074, -3.1272, -4.1054,  0.2828,  1.2584]],\n",
       "\n",
       "        [[ 1.7990, -3.8503, -1.8308, -0.9864, -0.0882],\n",
       "         [-0.8277, -1.0676, -0.4600, -1.3033, -0.7147],\n",
       "         [ 0.8170, -0.4777,  2.0906, -3.1950, -1.3288]],\n",
       "\n",
       "        [[ 1.4840,  0.2684, -2.5263,  2.0744,  3.6022],\n",
       "         [-0.3857, -0.6719, -0.6715, -0.7410, -0.5500],\n",
       "         [-2.7286, -0.4270, -1.7050, -2.5529, -0.9331]]]), OrderedDict([('batch2', Bint[4, ]), ('chans', Bint[3, ]), ('layer2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"batch\", Bint[4]), (\"chans\", Bint[3]), (\"layer\", Bint[5])]))\n",
    "g = random_tensor(OrderedDict([(\"chans\", Bint[3])]))\n",
    "b = random_tensor(OrderedDict([(\"chans\", Bint[3])]))\n",
    "\n",
    "BatchNorm(x, g, b, \"batch\", \"layer\", \"batch2\", \"layer2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[-0.3284,  2.1825, -3.7395, -0.2500, -0.5638],\n",
       "         [-0.7521, -0.2031, -1.1559, -0.7626, -0.3432],\n",
       "         [-4.2041, -0.3833,  0.6169, -0.0718, -1.4822]],\n",
       "\n",
       "        [[-0.4231,  0.2427, -3.5611,  2.2202, -1.1777],\n",
       "         [-0.6709, -0.7302, -0.7212, -0.0278, -1.0668],\n",
       "         [ 0.4637, -2.7675, -3.4967, -0.2255,  0.5017]],\n",
       "\n",
       "        [[ 2.2742, -3.4231, -1.3864, -0.5348,  0.3710],\n",
       "         [-0.5886, -0.8684, -0.1596, -1.1435, -0.4567],\n",
       "         [ 0.0504, -1.1600,  1.2410, -3.7002, -1.9556]],\n",
       "\n",
       "        [[-0.0780, -1.1931, -3.7567,  0.4636,  1.8650],\n",
       "         [-0.0538, -0.8267, -0.8256, -1.0133, -0.4975],\n",
       "         [-3.1105,  1.2472, -1.1724, -2.7777,  0.2889]]]), OrderedDict([('batch', Bint[4, ]), ('chans', Bint[3, ]), ('layer2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InstanceNorm(x, g, b, \"layer\", \"layer2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Relu(\n",
    "    X: Funsor\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.max(X, Number(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"chans\", Bint[3]),\n",
    "        (\"kh\", Bint[3]),\n",
    "        (\"kw\", Bint[4]),\n",
    "        (\"chans2\", Bint[3])\n",
    "    ]),\n",
    ")\n",
    "b1 = random_tensor(OrderedDict([(\"chans2\", Bint[3])]))\n",
    "W3 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden\", Bint[3]),\n",
    "        (\"height3\", Bint[4]),\n",
    "        (\"width3\", Bint[4]),\n",
    "        (\"chans2\", Bint[3])\n",
    "    ]),\n",
    ")\n",
    "b3 = random_tensor(OrderedDict([(\"hidden\", Bint[3])]))\n",
    "W4 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden\", Bint[3]),\n",
    "        (\"classes\", Bint[5]),\n",
    "    ]),\n",
    ")\n",
    "b4 = random_tensor(OrderedDict([(\"classes\", Bint[5])]))\n",
    "\n",
    "X0 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"batch\", Bint[4]),\n",
    "        (\"chans\", Bint[3]),\n",
    "        (\"height\", Bint[14]),\n",
    "        (\"width\", Bint[15])\n",
    "    ])\n",
    ")\n",
    "\n",
    "T1 = Relu(\n",
    "    Conv2d(X0, W1, b1, \"chans\", 3, \"kh\", \"height\", \"height2\", 4, \"kw\", \"width\", \"width2\")\n",
    ")\n",
    "X1 = MaxPool2d(T1, \"height2\", 3, \"kh\", \"height3\", \"width2\", 3, \"kw\", \"width3\")\n",
    "X3 = (W3 * X1).reduce(ops.add, frozenset({\"height3\", \"width3\", \"chans2\"})) + b3\n",
    "O = Softmax(((W4 * X3).reduce(ops.add, \"hidden\") + b4), \"classes\", \"classes2\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
