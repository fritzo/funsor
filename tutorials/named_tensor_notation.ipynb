{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: FUNSOR_TYPECHECK=1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import functools\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "%env FUNSOR_TYPECHECK=1\n",
    "import funsor\n",
    "from funsor.terms import Funsor, Variable, Number\n",
    "from funsor.tensor import Tensor\n",
    "from funsor.domains import Array, Bint, Real, Reals\n",
    "from funsor.factory import Bound, Fresh, Has, Value, make_funsor, to_funsor\n",
    "import funsor.ops as ops\n",
    "from funsor.cnf import Contraction\n",
    "from funsor.testing import random_tensor\n",
    "from funsor.interpretations import reflect\n",
    "\n",
    "funsor.set_backend(\"torch\")\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informal Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(\n",
    "    torch.tensor([[3., 1., 4.],\n",
    "                  [1., 5., 9.],\n",
    "                  [2., 6., 5.]])\n",
    ")[\"height\", \"width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([3., 1., 4.]), OrderedDict([('width', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A(height=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([4., 9., 5.]), OrderedDict([('height', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A(width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise operations and broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[0.9526, 0.7311, 0.9820],\n",
       "        [0.7311, 0.9933, 0.9999],\n",
       "        [0.8808, 0.9975, 0.9933]]), OrderedDict([('height', Bint[3, ]), ('width', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sigmoid()  # 1 / (1 + (-A).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(torch.tensor([2., 7., 1.]))[\"height\"]\n",
    "\n",
    "y = Tensor(torch.tensor([1., 4., 1.]))[\"width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[ 5.,  3.,  6.],\n",
       "        [ 8., 12., 16.],\n",
       "        [ 3.,  7.,  6.]]), OrderedDict([('height', Bint[3, ]), ('width', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[ 4.,  5.,  5.],\n",
       "        [ 2.,  9., 10.],\n",
       "        [ 3., 10.,  6.]]), OrderedDict([('height', Bint[3, ]), ('width', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([ 6., 12., 18.]), OrderedDict([('width', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce(ops.add, reduced_vars=\"height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([ 8., 15., 13.]), OrderedDict([('height', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce(ops.add, reduced_vars=\"width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(36.0, OrderedDict(), 'real')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce(ops.add, reduced_vars={\"height\", \"width\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([11., 30., 31.]), OrderedDict([('height', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A * y).reduce(ops.add, reduced_vars=\"width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([15., 43., 76.]), OrderedDict([('width', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x * x).reduce(ops.add, reduced_vars=\"height\")  # innder product\n",
    "x * y  # outer product\n",
    "(A * y).reduce(ops.add, reduced_vars=\"width\")  # matrix-vector product\n",
    "# vector-matrix product is the same as matrix-vector product\n",
    "(A * x).reduce(ops.add, reduced_vars=\"height\")  # vector-matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = Tensor(\n",
    "    torch.tensor([[3, 2, 5],\n",
    "                  [5, 4, 0],\n",
    "                  [8, 3, 6]]),\n",
    ")[\"width\", \"width2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[ 46.,  22.,  39.],\n",
       "        [100.,  49.,  59.],\n",
       "        [ 76.,  43.,  40.]]), OrderedDict([('height', Bint[3, ]), ('width2', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A * B).reduce(ops.add, reduced_vars=\"width\")  # matrix-matrix product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[3., 1., 4.],\n",
       "        [1., 5., 9.],\n",
       "        [2., 6., 5.]]), OrderedDict([('height2', Bint[3, ]), ('width', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A(height=\"height2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "X0 = random_tensor(\n",
    "    OrderedDict([(\"input_layer\", Bint[input_dim])])\n",
    ")\n",
    "\n",
    "hidden_1_dim = 32\n",
    "W1 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"input_layer\", Bint[input_dim]),\n",
    "        (\"hidden_layer_1\", Bint[hidden_1_dim])\n",
    "    ])\n",
    ")\n",
    "b1 = random_tensor(\n",
    "    OrderedDict([(\"hidden_layer_1\", Bint[hidden_1_dim])])\n",
    ")\n",
    "X1 = ((W1 * X0).reduce(ops.add, \"input_layer\") + b1).sigmoid()\n",
    "\n",
    "hidden_2_dim = 16\n",
    "W2 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden_layer_1\", Bint[hidden_1_dim]),\n",
    "        (\"hidden_layer_2\", Bint[hidden_2_dim])\n",
    "    ])\n",
    ")\n",
    "b2 = random_tensor(\n",
    "    OrderedDict([(\"hidden_layer_2\", Bint[hidden_2_dim])])\n",
    ")\n",
    "X2 = ((W2 * X1).reduce(ops.add, \"hidden_layer_1\") + b2).sigmoid()\n",
    "\n",
    "hidden_3_dim = 8\n",
    "W3 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden_layer_2\", Bint[hidden_2_dim]),\n",
    "        (\"hidden_layer_3\", Bint[hidden_3_dim])\n",
    "    ])\n",
    ")\n",
    "b3 = random_tensor(\n",
    "    OrderedDict([(\"hidden_layer_3\", Bint[hidden_3_dim])])\n",
    ")\n",
    "X3 = ((W3 * X2).reduce(ops.add, \"hidden_layer_2\") + b3).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def FullConnLayer(\n",
    "    x: Has[{\"layer\"}],\n",
    "    W: Has[{\"layer\"}],\n",
    "    b: Funsor,\n",
    "    layer: Bound\n",
    ") -> Fresh[lambda x: x]:\n",
    "    result = ((W * x).reduce(ops.add, layer) + b).sigmoid()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([1.0000e+00, 9.6420e-04, 5.9377e-01, 9.9878e-01, 3.0042e-03, 9.8825e-01,\n",
       "        6.0447e-03, 8.1884e-04, 9.0660e-02, 9.9972e-01, 4.1371e-01, 8.4666e-01,\n",
       "        9.9900e-01, 2.1682e-01, 8.3070e-02, 2.3368e-02, 5.5098e-03, 5.9357e-01,\n",
       "        1.9522e-04, 9.7225e-01, 7.8749e-03, 1.7118e-01, 1.0000e+00, 1.0000e+00,\n",
       "        4.8502e-01, 5.7865e-04, 1.0000e+00, 8.2699e-05, 9.9985e-01, 4.4409e-06,\n",
       "        6.7093e-01, 1.6391e-08]), OrderedDict([('out_layer', Bint[32, ])]), 'real')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 100\n",
    "X0 = random_tensor(\n",
    "    OrderedDict([(\"layer\", Bint[input_dim])])\n",
    ")\n",
    "hidden_1_dim = 32\n",
    "W1 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"layer\", Bint[input_dim]),\n",
    "        (\"out_layer\", Bint[hidden_1_dim])\n",
    "    ])\n",
    ")\n",
    "b1 = random_tensor(\n",
    "    OrderedDict([(\"out_layer\", Bint[hidden_1_dim])])\n",
    ")\n",
    "\n",
    "X1 = FullConnLayer(X0, W1, b1, \"layer\")\n",
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def RecurrentLayer(\n",
    "    x: Funsor,\n",
    "    Wh: Funsor,\n",
    "    Wi: Funsor,\n",
    "    b: Funsor,\n",
    "    hidden: Bound,\n",
    "    input: Bound\n",
    ") -> Fresh[lambda x: x]:\n",
    "    output = ((Wh * h).reduce(ops.add, \"hidden\") + (Wi * x).reduce(ops.add, \"input\") + b).sigmoid()\n",
    "    return output(hidden=\"new_hidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Softmax(\n",
    "    x: Funsor,\n",
    "    ax: Bound,\n",
    "    ax2: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    x = x(**{ax.name: ax2.name})\n",
    "    y = x - x.reduce(ops.logaddexp, ax2)\n",
    "    return y.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([0.2437, 0.0257, 0.0642, 0.1169, 0.0384, 0.1301, 0.0492, 0.1176, 0.0133,\n",
       "        0.2010]), OrderedDict([('key2', Bint[10, ])]), 'real')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = random_tensor(OrderedDict([(\"key\", Bint[10])]))\n",
    "Softmax(q, \"key\", \"key2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Attention(\n",
    "    Q: Has[{\"key\"}],\n",
    "    K: Has[{\"key\", \"seq\"}],\n",
    "    V: Has[{\"seq2\"}],\n",
    "    M: Has[{\"seq\"}],\n",
    "    key: Bound,\n",
    "    seq: Bound,\n",
    "    seq2: Bound\n",
    ") -> Fresh[lambda Q: Q]:\n",
    "    x = (Q * K).reduce(ops.add, key) / math.sqrt(key.output.size) + M\n",
    "    return (Softmax(x, seq, seq2) * V).reduce(ops.add, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([-0.8274, -1.4611, -0.2073, -0.7946,  0.5313]), OrderedDict([('val', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = random_tensor(OrderedDict([(\"key\", Bint[10])]))\n",
    "k = random_tensor(OrderedDict([(\"key\", Bint[10]), (\"seq\", Bint[3])]))\n",
    "v = random_tensor(OrderedDict([(\"seq2\", Bint[3]), (\"val\", Bint[5])]))\n",
    "m = random_tensor(OrderedDict([(\"seq\", Bint[3])]))\n",
    "Attention(q, k, v, m, \"key\", \"seq\", \"seq2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Unroll(\n",
    "    x: Funsor,\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size - k + 1]]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    return x(**{seq.name: seq2 + kernel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Unroll2(\n",
    "    x: Funsor,\n",
    "    seq: Bound,\n",
    "    kernel: Funsor,\n",
    "    seq2: Fresh[lambda seq, kernel: Bint[seq.size - kernel.size + 1]]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    return x(**{seq.name: seq2 + kernel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[-2.1018,  1.1907, -0.8056],\n",
       "         [ 1.1907, -0.8056, -1.1722],\n",
       "         [-0.8056, -1.1722,  0.5730],\n",
       "         [-1.1722,  0.5730, -0.1003],\n",
       "         [ 0.5730, -0.1003, -0.4544],\n",
       "         [-0.1003, -0.4544, -0.0859],\n",
       "         [-0.4544, -0.0859, -1.4297],\n",
       "         [-0.0859, -1.4297,  0.2003]],\n",
       "\n",
       "        [[ 1.0350, -0.2168,  1.6012],\n",
       "         [-0.2168,  1.6012,  1.5195],\n",
       "         [ 1.6012,  1.5195,  1.2746],\n",
       "         [ 1.5195,  1.2746, -2.1400],\n",
       "         [ 1.2746, -2.1400,  0.2788],\n",
       "         [-2.1400,  0.2788,  0.0247],\n",
       "         [ 0.2788,  0.0247, -0.2465],\n",
       "         [ 0.0247, -0.2465, -0.9741]],\n",
       "\n",
       "        [[ 0.0922, -0.5518, -0.9372],\n",
       "         [-0.5518, -0.9372,  0.3317],\n",
       "         [-0.9372,  0.3317,  0.7012],\n",
       "         [ 0.3317,  0.7012,  0.4599],\n",
       "         [ 0.7012,  0.4599,  0.4827],\n",
       "         [ 0.4599,  0.4827, -1.1506],\n",
       "         [ 0.4827, -1.1506,  1.1231],\n",
       "         [-1.1506,  1.1231, -1.5177]]]), OrderedDict([('chans', Bint[3, ]), ('seq2', Bint[8, ]), ('kernel', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"seq\", Bint[10])]))\n",
    "Y = Unroll2(X, \"seq\", Variable(\"kernel\", Bint[3]), \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Conv1d(\n",
    "    X: Has[{\"chans\", \"seq\"}],\n",
    "    W: Has[{\"chans\", \"kernel\"}],\n",
    "    b: Funsor,\n",
    "    chans: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Bound,\n",
    "    seq: Bound,\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size - k + 1]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = W * Unroll(X, seq, k, kernel, seq2)\n",
    "    return y.reduce(ops.add, frozenset({chans, kernel})) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"seq\", Bint[10])]))\n",
    "kernel = Variable(\"kernel\", Bint[3])\n",
    "w = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"kernel\", Bint[3])]))\n",
    "b = random_tensor(OrderedDict([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([-0.6673,  1.9524, -1.1526, -2.9487,  4.3021,  1.8157, -0.9582, -1.5912]), OrderedDict([('seq2', Bint[8, ])]), 'real')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv1d(x, w, b, \"chans\", 3, \"kernel\", \"seq\", \"seq2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Conv2d(\n",
    "    X: Has[{\"chans\", \"height\", \"width\"}],\n",
    "    W: Has[{\"chans\", \"kh\", \"kw\"}],\n",
    "    b: Funsor,\n",
    "    chans: Bound,\n",
    "    kh_size: Value[int],\n",
    "    kh: Bound,\n",
    "    height: Bound,\n",
    "    height2: Fresh[lambda height, kh_size: Bint[height.size - kh_size + 1]],\n",
    "    kw_size: Value[int],\n",
    "    kw: Bound,\n",
    "    width: Bound,\n",
    "    width2: Fresh[lambda width, kw_size: Bint[width.size - kw_size + 1]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = W * Unroll(Unroll(X, width, kw_size, kw, width2), height, kh_size, kh, height2)\n",
    "    return y.reduce(ops.add, frozenset({chans, kh, kw})) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[  3.8090,  -1.9533,   7.4257,  -0.8424,  -3.7585],\n",
       "        [-10.2185,   0.3733,   1.8906,   1.1670, -10.3593],\n",
       "        [-11.2712,  -0.9658,  -6.6962,   6.9607,   7.3948],\n",
       "        [ -2.3084,  -3.1277,   3.3852,  -2.3263,   0.1138],\n",
       "        [-16.6359,   2.2360,  12.6339,  -2.7315,   0.9199],\n",
       "        [  3.5712,   0.5392,  -8.0635, -11.6026,   1.2325],\n",
       "        [ 14.3903,   3.6469,   4.7069,  -4.9921, -15.5302],\n",
       "        [ 15.0121,  -1.5841,   0.6507,  -6.7394,  -0.1217]]), OrderedDict([('height2', Bint[8, ]), ('width2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"height\", Bint[10]), (\"width\", Bint[8])]))\n",
    "w = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"kh\", Bint[3]), (\"kw\", Bint[4])]))\n",
    "b = random_tensor(OrderedDict([]))\n",
    "\n",
    "Conv2d(x, w, b, \"chans\", 3, \"kh\", \"height\", \"height2\", 4, \"kw\", \"width\", \"width2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Pool(\n",
    "    x: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]], # seq -> Bint[]\n",
    ") -> Fresh[lambda x: x]: # x -> x.output (Bint[] or Real)\n",
    "    assert not seq.output.size % k\n",
    "    return x(**{seq.name: seq2 * Number(k, k+1) + kernel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[-1.3082, -1.4157],\n",
       "        [-0.2013,  1.0847],\n",
       "        [ 1.3470, -0.3982],\n",
       "        [ 0.1646,  0.3407],\n",
       "        [-0.1591,  0.1409]]), OrderedDict([('seq2', Bint[5, ]), ('kernel', Bint[2, ])]), 'real')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = Pool(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool1d(\n",
    "    X: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Pool(X, seq, k, kernel, seq2).reduce(ops.max, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([3.0786, 0.7304, 1.4282, 1.7233, 1.3209]), OrderedDict([('seq2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = MaxPool1d(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool2d(\n",
    "    X: Has[{\"height\", \"width\"}],\n",
    "    height: Bound,\n",
    "    kh_size: Value[int],\n",
    "    kh: Fresh[lambda kh_size: Bint[kh_size]],\n",
    "    height2: Fresh[lambda height, kh_size: Bint[height.size // kh_size]],\n",
    "    width: Bound,\n",
    "    kw_size: Value[int],\n",
    "    kw: Fresh[lambda kw_size: Bint[kw_size]],\n",
    "    width2: Fresh[lambda width, kw_size: Bint[width.size // kw_size]],\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = Pool(Pool(X, height, kh_size, kh, height2), width, kw_size, kw, width2)\n",
    "    return y.reduce(ops.max, frozenset({kh, kw}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[1.1734, 0.5261],\n",
       "        [1.3100, 1.4246],\n",
       "        [0.9372, 1.0184]]), OrderedDict([('width2', Bint[3, ]), ('height2', Bint[2, ])]), 'real')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"width\", Bint[9]), (\"height\", Bint[4])]))\n",
    "Y = MaxPool2d(X, \"height\", 2, \"kh\", \"height2\", \"width\", 3, \"kw\", \"width2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Pool2(\n",
    "    x: Funsor,\n",
    "    seq: Bound,\n",
    "    kernel: Funsor,\n",
    "    seq2: Fresh[lambda seq, kernel: Bint[seq.size // kernel.size]], # seq -> Bint[]\n",
    ") -> Fresh[lambda x: x]: # x -> x.output (Bint[] or Real)\n",
    "    return x(**{seq.name: seq2 * Number(kernel.output.size, kernel.output.size+1) + kernel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "x.sum(dim=0)\n",
    "x.mean(dim=0)\n",
    "\n",
    "# Funsor\n",
    "x.reduce(ops.add, \"i\")\n",
    "x.reduce(ops.mean, \"i\") # Wrong\n",
    "x.mean(\"i\")\n",
    "\n",
    "@make_funsor\n",
    "def Mean(\n",
    "    X: Funsor,\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.mean(funsor.terms.Lambda(X, ax), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Mean(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, ax) / ax.output.size\n",
    "\n",
    "@make_funsor\n",
    "def Mean2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, frozenset({ax, ax2})) / (ax.output.size * ax2.output.size)\n",
    "\n",
    "@make_funsor\n",
    "def Variance(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean((X - Mean(X, ax))**2, ax)\n",
    "\n",
    "\n",
    "@make_funsor\n",
    "def Variance2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean2((X - Mean2(X, ax, ax2))**2, ax, ax2)\n",
    "\n",
    "@make_funsor\n",
    "def Standardize(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax})\n",
    "    return (y - Mean(X, ax)) / (Variance(X, ax) + ops.finfo(X.data).eps).sqrt()\n",
    "\n",
    "@make_funsor\n",
    "def Standardize2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax],\n",
    "    new_ax2: Fresh[lambda ax2: ax2]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax, ax2.name: new_ax2})\n",
    "    return (y - Mean2(X, ax, ax2)) / (Variance2(X, ax, ax2) + ops.finfo(X.data).eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def BatchNorm(\n",
    "    X: Has[{\"batch\", \"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    batch: Bound,\n",
    "    layer: Bound,\n",
    "    batch2: Fresh[lambda batch: batch],\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize2(X, batch, layer, batch2, layer2) * gamma + beta\n",
    "\n",
    "@make_funsor\n",
    "def InstanceNorm(\n",
    "    X: Has[{\"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    layer: Bound,\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize(X, layer, layer2) * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[ 1.4904,  1.4674,  1.4772,  1.4581,  1.4745],\n",
       "         [ 1.3085,  1.4470,  1.3362,  1.3710,  1.3120],\n",
       "         [-0.4153, -0.5301, -0.5862, -0.4013, -0.7001]],\n",
       "\n",
       "        [[ 1.4705,  1.4701,  1.4662,  1.4754,  1.4709],\n",
       "         [ 1.3233,  1.2909,  1.3478,  1.2751,  1.3455],\n",
       "         [-0.6072, -0.2954, -0.2990, -0.4556, -0.8701]],\n",
       "\n",
       "        [[ 1.4698,  1.4679,  1.4770,  1.4833,  1.4904],\n",
       "         [ 1.3641,  1.3555,  1.3911,  1.3633,  1.3066],\n",
       "         [-0.7784, -0.6248, -0.5895, -0.3076, -0.3127]],\n",
       "\n",
       "        [[ 1.4795,  1.4730,  1.4691,  1.4822,  1.4694],\n",
       "         [ 1.4017,  1.3826,  1.3396,  1.3068,  1.3966],\n",
       "         [-0.1873, -0.4955, -0.4423, -0.1039, -0.5801]]]), OrderedDict([('batch2', Bint[4, ]), ('chans', Bint[3, ]), ('layer2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"batch\", Bint[4]), (\"chans\", Bint[3]), (\"layer\", Bint[5])]))\n",
    "g = random_tensor(OrderedDict([(\"chans\", Bint[3])]))\n",
    "b = random_tensor(OrderedDict([(\"chans\", Bint[3])]))\n",
    "\n",
    "BatchNorm(x, g, b, \"batch\", \"layer\", \"batch2\", \"layer2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[ 1.4865,  1.4696,  1.4768,  1.4628,  1.4748],\n",
       "         [ 1.3104,  1.4234,  1.3330,  1.3614,  1.3132],\n",
       "         [-0.2882, -0.4851, -0.5813, -0.2642, -0.7768]],\n",
       "\n",
       "        [[ 1.4739,  1.4728,  1.4622,  1.4869,  1.4748],\n",
       "         [ 1.3580,  1.3115,  1.3931,  1.2889,  1.3898],\n",
       "         [-0.5691, -0.2935, -0.2966, -0.4350, -0.8015]],\n",
       "\n",
       "        [[ 1.4667,  1.4649,  1.4735,  1.4794,  1.4860],\n",
       "         [ 1.3604,  1.3473,  1.4013,  1.3591,  1.2732],\n",
       "         [-0.7431, -0.5846, -0.5482, -0.2573, -0.2625]],\n",
       "\n",
       "        [[ 1.4813,  1.4718,  1.4659,  1.4853,  1.4664],\n",
       "         [ 1.3897,  1.3678,  1.3188,  1.2813,  1.3838],\n",
       "         [-0.2981, -0.6177, -0.5626, -0.2117, -0.7055]]]), OrderedDict([('batch', Bint[4, ]), ('chans', Bint[3, ]), ('layer2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InstanceNorm(x, g, b, \"layer\", \"layer2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete random variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_given_A = Tensor(\n",
    "    torch.tensor([[0.2, 0.3, 0.5],\n",
    "                  [0.8, 0.1, 0.1]]),\n",
    ")[\"A\", \"B\"]\n",
    "\n",
    "A = Tensor(\n",
    "    torch.tensor([0.6, 0.4]),\n",
    ")[\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[0.1200, 0.1800, 0.3000],\n",
       "        [0.3200, 0.0400, 0.0400]]), OrderedDict([('A', Bint[2, ]), ('B', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_and_B = B_given_A * A\n",
    "A_and_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([0.4400, 0.2200, 0.3400]), OrderedDict([('B', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "B = A_and_B.reduce(ops.add, \"A\")\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[0.2727, 0.8182, 0.8824],\n",
       "        [0.7273, 0.1818, 0.1176]]), OrderedDict([('A', Bint[2, ]), ('B', Bint[3, ])]), 'real')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_given_B = A_and_B / B\n",
    "A_given_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sudoku ILP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random_tensor(OrderedDict([(\"batch\", Bint[10]), (\"d\", Bint[4])]))\n",
    "c = random_tensor(OrderedDict([(\"clusters\", Bint[3]), (\"d\", Bint[4])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTeX Macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formal Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
