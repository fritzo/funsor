{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: FUNSOR_TYPECHECK=1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import functools\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "%env FUNSOR_TYPECHECK=1\n",
    "import funsor\n",
    "from funsor.terms import Funsor, Variable, Number, Lambda, Slice\n",
    "from funsor.tensor import Tensor\n",
    "from funsor.domains import Array, Bint, Real, Reals\n",
    "from funsor.factory import Bound, Fresh, Has, Value, make_funsor, to_funsor\n",
    "import funsor.ops as ops\n",
    "from funsor.cnf import Contraction\n",
    "from funsor.testing import random_tensor\n",
    "from funsor.interpretations import reflect, memoize\n",
    "import funsor.torch.distributions as dist\n",
    "\n",
    "funsor.set_backend(\"torch\")\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  X^0 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}} \\\\\n",
    "  X^1 &= \\sigma(W^1 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}input}}}{\\vphantom{fg}\\odot}} X^0 + b^1) & W^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1 \\times \\mathsf{\\vphantom{fg}input}} & b^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1} \\\\\n",
    "  X^2 &= \\sigma(W^2 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_1}}{\\vphantom{fg}\\odot}} X^1 + b^2) & W^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2 \\times \\mathsf{\\vphantom{fg}hidden}_1} & b^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2} \\\\\n",
    "  X^3 &= \\sigma(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_2}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}\\times \\mathsf{\\vphantom{fg}hidden}_2} & b^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConnLayer(Layer):\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        self.W = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"input\", Bint[input_size]),\n",
    "                (\"output\", Bint[output_size])\n",
    "            ])\n",
    "        )\n",
    "        self.W.data.requires_grad = True\n",
    "        \n",
    "        self.b = random_tensor(\n",
    "            OrderedDict([(\"output\", Bint[output_size])])\n",
    "        )\n",
    "        self.b.data.requires_grad = True\n",
    "        \n",
    "    def forward(self, x: Funsor) -> Funsor:\n",
    "        out = ops.sigmoid((self.W * x).reduce(ops.add, \"input\") + self.b)\n",
    "        return out(**{\"output\": \"input\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullConn1 = FullConnLayer(100, 32)\n",
    "FullConn2 = FullConnLayer(32, 16)\n",
    "FullConn3 = FullConnLayer(16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([0.3556, 0.5652, 0.9982, 0.3874, 0.9996, 0.7776, 0.8879, 0.9356],\n",
       "       grad_fn=<SigmoidBackward>), OrderedDict([('input', Bint[8, ])]), 'real')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = random_tensor(OrderedDict([(\"input\", Bint[100])]))\n",
    "X1 = FullConn1(X0)\n",
    "X2 = FullConn2(X1)\n",
    "X3 = FullConn3(X2)\n",
    "X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "x^{t} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}} & t &= 1, \\ldots, n \\\\\n",
    "W^{\\text{h}} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}hidden}^\\prime} & |\\mathsf{\\vphantom{fg}hidden}| &= |\\mathsf{\\vphantom{fg}hidden}^\\prime| \\\\\n",
    "W^{\\text{i}} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}\\times \\mathsf{\\vphantom{fg}hidden}^\\prime} \\\\\n",
    "b &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}^\\prime} \\\\\n",
    "h^{0} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}} \\\\\n",
    "h^{t} &= \\sigma\\left( W^{\\text{h}} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}}}{\\vphantom{fg}\\odot}} h^{t-1} + W^{\\text{i}} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}input}}}{\\vphantom{fg}\\odot}} x^{t} + b \\right)_{\\mathsf{\\vphantom{fg}hidden}^\\prime\\rightarrow\\mathsf{\\vphantom{fg}hidden}} & t &= 1, \\ldots, n\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer(Layer):\n",
    "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
    "        self.Wh = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"hidden\", Bint[hidden_size]),\n",
    "                (\"hidden2\", Bint[hidden_size])\n",
    "            ])\n",
    "        )\n",
    "        self.Wh.data.requires_grad = True\n",
    "        \n",
    "        self.Wi = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"input\", Bint[input_size]),\n",
    "                (\"hidden2\", Bint[hidden_size])\n",
    "            ])\n",
    "        )\n",
    "        self.Wi.data.requires_grad = True\n",
    "        \n",
    "        self.b = random_tensor(\n",
    "            OrderedDict([(\"hidden2\", Bint[hidden_size])])\n",
    "        )\n",
    "        self.b.data.requires_grad = True\n",
    "        \n",
    "    def forward(self, x: Funsor, h: Funsor) -> Funsor:\n",
    "        out = ops.sigmoid(\n",
    "            (self.Wh * h).reduce(ops.add, \"hidden\") + (self.Wi * x).reduce(ops.add, \"input\") + b\n",
    "        )\n",
    "        return out(**{\"hidden2\": \"hidden\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Softmax(\n",
    "    x: Funsor,\n",
    "    ax: Bound,\n",
    "    ax2: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    x = x(**{ax.name: ax2.name})\n",
    "    y = x - x.reduce(ops.logaddexp, ax2)\n",
    "    return y.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\text{Attention} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}val}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}val}} \\\\\n",
    "\\text{Attention}(Q, K, V, M) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\mathrm{softmax}}} \\left( \\frac{Q \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}key}}}{\\vphantom{fg}\\odot}} K}{\\sqrt{|\\mathsf{\\vphantom{fg}key}|}} + M \\right) \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\odot}} V.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Attention(\n",
    "    Q: Has[{\"key\"}],\n",
    "    K: Has[{\"key\", \"seq\"}],\n",
    "    V: Has[{\"seq2\"}],\n",
    "    M: Has[{\"seq\"}],\n",
    "    key: Bound,\n",
    "    seq: Bound,\n",
    "    seq2: Bound\n",
    ") -> Fresh[lambda Q: Q]:\n",
    "    x = (Q * K).reduce(ops.add, key) / math.sqrt(key.output.size) + M\n",
    "    return (Softmax(x, seq, seq2) * V).reduce(ops.add, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([-0.2815,  1.1418, -0.1922, -1.3501, -1.0335]), OrderedDict([('val', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = random_tensor(OrderedDict([(\"key\", Bint[10])]))\n",
    "k = random_tensor(OrderedDict([(\"key\", Bint[10]), (\"seq\", Bint[3])]))\n",
    "v = random_tensor(OrderedDict([(\"seq2\", Bint[3]), (\"val\", Bint[5])]))\n",
    "m = random_tensor(OrderedDict([(\"seq\", Bint[3])]))\n",
    "Attention(q, k, v, m, \"key\", \"seq\", \"seq2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n-|\\mathsf{\\vphantom{fg}kernel}|+1], \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X &= Y,\\ \\text{where} \\\\\n",
    "  Y_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}kernel}(j)} &= X_{\\mathsf{\\vphantom{fg}seq}(i+j - 1)}.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Unroll(\n",
    "    x: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size - k + 1]]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    return x(**{seq.name: seq2 + kernel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\text{Conv1d} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n^\\prime]} \\\\\n",
    "\\text{Conv1d}(X; W, b) &= W \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\odot}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X + b\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "W &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "b &\\in \\mathbb{R}\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(Layer):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int) -> None:\n",
    "        self.W = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[in_channels]),\n",
    "                (\"chans2\", Bint[out_channels]),\n",
    "                (\"kernel\", Bint[kernel_size])\n",
    "            ])\n",
    "        )\n",
    "        self.W.data.requires_grad = True\n",
    "        \n",
    "        self.b = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans2\", Bint[out_channels])\n",
    "            ])\n",
    "        )\n",
    "        self.b.data.requires_grad = True\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def forward(self, x: Funsor) -> Funsor:\n",
    "        unrolled_x = Unroll(x, \"seq\", self.kernel_size, \"kernel\", \"seq2\")\n",
    "        out = (self.W * unrolled_x).reduce(ops.add, {\"chans\", \"kernel\"}) + self.b\n",
    "        return out(**{\"chans2\": \"chans\", \"seq2\": \"seq\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(Layer):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kh_size: int, kw_size: int) -> None:\n",
    "        self.W = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[in_channels]),\n",
    "                (\"chans2\", Bint[out_channels]),\n",
    "                (\"kh\", Bint[kh_size]),\n",
    "                (\"kw\", Bint[Kw_size])\n",
    "            ])\n",
    "        )\n",
    "        self.W.data.requires_grad = True\n",
    "        \n",
    "        self.b = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans2\", Bint[out_channels])\n",
    "            ])\n",
    "        )\n",
    "        self.b.data.requires_grad = True\n",
    "        \n",
    "        self.kh_size = kh_size\n",
    "        self.kw_size = kw_size\n",
    "        \n",
    "    def forward(self, x: Funsor) -> Funsor:\n",
    "        unrolled_w_x = Unroll(x, \"width\", self.kw_size, \"kw\", \"width2\")\n",
    "        unrolled_hw_x = Unroll(unrolled_w_x, \"height\", self.kh_size, \"kh\", \"height2\")\n",
    "        out = (self.W * unrolled_hw_x).reduce(ops.add, {\"chans\", \"kh\", \"kw\"}) + self.b\n",
    "        return out(**{\"chans2\": \"chans\", \"height2\": \"height\", \"width2\": \"width\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Pool(\n",
    "    x: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]], # seq -> Bint[]\n",
    ") -> Fresh[lambda x: x]: # x -> x.output (Bint[] or Real)\n",
    "    assert not seq.output.size % k\n",
    "    return x(**{seq.name: seq2 * Number(k, k+1) + kernel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[ 1.5662,  0.0141],\n",
       "        [-0.2258,  0.8933],\n",
       "        [ 0.0305, -0.3126],\n",
       "        [-0.4944, -2.0142],\n",
       "        [-0.3566,  0.6063]]), OrderedDict([('seq2', Bint[5, ]), ('kernel', Bint[2, ])]), 'real')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = Pool(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool1d(\n",
    "    X: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Pool(X, seq, k, kernel, seq2).reduce(ops.max, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([ 0.8018, -0.1132,  0.6756,  0.9458,  0.3273]), OrderedDict([('seq2', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = MaxPool1d(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool2d(\n",
    "    X: Has[{\"height\", \"width\"}],\n",
    "    height: Bound,\n",
    "    kh_size: Value[int],\n",
    "    kh: Fresh[lambda kh_size: Bint[kh_size]],\n",
    "    height2: Fresh[lambda height, kh_size: Bint[height.size // kh_size]],\n",
    "    width: Bound,\n",
    "    kw_size: Value[int],\n",
    "    kw: Fresh[lambda kw_size: Bint[kw_size]],\n",
    "    width2: Fresh[lambda width, kw_size: Bint[width.size // kw_size]],\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = Pool(Pool(X, height, kh_size, kh, height2), width, kw_size, kw, width2)\n",
    "    return y.reduce(ops.max, frozenset({kh, kw}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[0.2411, 0.4493],\n",
       "        [1.9085, 1.4201],\n",
       "        [1.8092, 1.9324]]), OrderedDict([('width2', Bint[3, ]), ('height2', Bint[2, ])]), 'real')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"width\", Bint[9]), (\"height\", Bint[4])]))\n",
    "Y = MaxPool2d(X, \"height\", 2, \"kh\", \"height2\", \"width\", 3, \"kw\", \"width2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "@make_funsor\n",
    "def Mean(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.mean(funsor.terms.Lambda(ax, X), 0)\n",
    "\n",
    "@make_funsor\n",
    "def Variance(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.var(funsor.terms.Lambda(ax, X), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "@make_funsor\n",
    "def Mean(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, ax) / ax.output.size\n",
    "\n",
    "@make_funsor\n",
    "def Mean2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, frozenset({ax, ax2})) / (ax.output.size * ax2.output.size)\n",
    "\n",
    "@make_funsor\n",
    "def Variance(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean((X - Mean(X, ax))**2, ax)\n",
    "\n",
    "\n",
    "@make_funsor\n",
    "def Variance2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean2((X - Mean2(X, ax, ax2))**2, ax, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Standardize(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax})\n",
    "    return (y - Mean(X, ax)) / (Variance(X, ax) + ops.finfo(X.data).eps).sqrt()\n",
    "\n",
    "@make_funsor\n",
    "def Standardize2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax],\n",
    "    new_ax2: Fresh[lambda ax2: ax2]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax, ax2.name: new_ax2})\n",
    "    return (y - Mean2(X, ax, ax2)) / (Variance2(X, ax, ax2) + ops.finfo(X.data).eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, num_channels: int) -> None:\n",
    "        self.beta = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[num_channels])\n",
    "            ])\n",
    "        )\n",
    "        self.beta.data.requires_grad = True\n",
    "        \n",
    "        self.gamma = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[num_channels])\n",
    "            ])\n",
    "        )\n",
    "        self.gamma.data.requires_grad = True\n",
    "        \n",
    "    def forward(self, x: Funsor) -> Funsor:\n",
    "        out = Standardize2(x, \"batch\", \"layer\", \"batch2\", \"layer2\") * self.gamma + self.beta\n",
    "        return out(**{\"batch2\": \"batch\", \"layer2\": \"layer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNorm(Layer):\n",
    "    def __init__(self, num_channels: int) -> None:\n",
    "        self.beta = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[num_channels])\n",
    "            ])\n",
    "        )\n",
    "        self.beta.data.requires_grad = True\n",
    "        \n",
    "        self.gamma = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[num_channels])\n",
    "            ])\n",
    "        )\n",
    "        self.gamma.data.requires_grad = True\n",
    "        \n",
    "    def forward(self, x: Funsor) -> Funsor:\n",
    "        out = Standardize(x, \"layer\", \"layer2\") * self.gamma + self.beta\n",
    "        return out(**{\"layer2\": \"layer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(Layer):\n",
    "    def __init__(self, num_channels: int, num_layers: int) -> None:\n",
    "        self.beta = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[num_channels]),\n",
    "                (\"layer\", Bint[num_layers])\n",
    "            ])\n",
    "        )\n",
    "        self.beta.data.requires_grad = True\n",
    "        \n",
    "        self.gamma = random_tensor(\n",
    "            OrderedDict([\n",
    "                (\"chans\", Bint[num_channels]),\n",
    "                (\"layer\", Bint[num_layers])\n",
    "            ])\n",
    "        )\n",
    "        self.gamma.data.requires_grad = True\n",
    "        \n",
    "    def forward(self, x: Funsor) -> Funsor:\n",
    "        out = Standardize2(x, \"chans\", \"layer\", \"chans2\", \"layer2\") * self.gamma + self.beta\n",
    "        return out(**{\"chans2\": \"chans\", \"layer2\": \"layer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[-1.2064, -0.5113, -0.8763, -1.1434, -1.0650],\n",
       "         [-0.7746, -1.4747,  0.1591,  0.2064,  1.0186],\n",
       "         [ 1.2582,  1.2571,  1.2567,  1.2565,  1.2605]],\n",
       "\n",
       "        [[-0.7646, -0.7682, -1.0567, -0.5184, -1.1284],\n",
       "         [-0.9481, -0.8563, -0.9254, -0.9457, -1.7601],\n",
       "         [ 1.2558,  1.2576,  1.2577,  1.2588,  1.2579]],\n",
       "\n",
       "        [[-1.4398, -0.4638, -0.6619, -0.4812, -0.9846],\n",
       "         [ 0.2047, -1.2126,  0.2145,  0.3439, -2.3063],\n",
       "         [ 1.2588,  1.2566,  1.2576,  1.2594,  1.2609]],\n",
       "\n",
       "        [[-0.6510, -0.7805, -0.6757, -0.4585, -1.0590],\n",
       "         [-0.5686,  1.1107, -0.9664, -0.7939, -0.1247],\n",
       "         [ 1.2577,  1.2584,  1.2576,  1.2582,  1.2606]]],\n",
       "       grad_fn=<AddBackward0>), OrderedDict([('batch', Bint[4, ]), ('chans', Bint[3, ]), ('layer', Bint[5, ])]), 'real')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"batch\", Bint[4]), (\"chans\", Bint[3]), (\"layer\", Bint[5])]))\n",
    "\n",
    "BatchNorm(3)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Relu(\n",
    "    X: Funsor\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.max(X, Number(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"chans\", Bint[3]),\n",
    "        (\"kh\", Bint[3]),\n",
    "        (\"kw\", Bint[4]),\n",
    "        (\"chans2\", Bint[3])\n",
    "    ]),\n",
    ")\n",
    "b1 = random_tensor(OrderedDict([(\"chans2\", Bint[3])]))\n",
    "W3 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden\", Bint[3]),\n",
    "        (\"height3\", Bint[4]),\n",
    "        (\"width3\", Bint[4]),\n",
    "        (\"chans2\", Bint[3])\n",
    "    ]),\n",
    ")\n",
    "b3 = random_tensor(OrderedDict([(\"hidden\", Bint[3])]))\n",
    "W4 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden\", Bint[3]),\n",
    "        (\"classes\", Bint[5]),\n",
    "    ]),\n",
    ")\n",
    "b4 = random_tensor(OrderedDict([(\"classes\", Bint[5])]))\n",
    "\n",
    "X0 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"batch\", Bint[4]),\n",
    "        (\"chans\", Bint[3]),\n",
    "        (\"height\", Bint[14]),\n",
    "        (\"width\", Bint[15])\n",
    "    ])\n",
    ")\n",
    "\n",
    "T1 = Relu(\n",
    "    Conv2d(X0, W1, b1, \"chans\", 3, \"kh\", \"height\", \"height2\", 4, \"kw\", \"width\", \"width2\")\n",
    ")\n",
    "X1 = MaxPool2d(T1, \"height2\", 3, \"kh\", \"height3\", \"width2\", 3, \"kw\", \"width3\")\n",
    "X3 = (W3 * X1).reduce(ops.add, frozenset({\"height3\", \"width3\", \"chans2\"})) + b3\n",
    "O = Softmax(((W4 * X3).reduce(ops.add, \"hidden\") + b4), \"classes\", \"classes2\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
