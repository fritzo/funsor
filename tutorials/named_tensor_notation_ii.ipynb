{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: FUNSOR_TYPECHECK=1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import functools\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "%env FUNSOR_TYPECHECK=1\n",
    "import funsor\n",
    "from funsor.terms import Funsor, Variable, Number, Lambda, Slice\n",
    "from funsor.tensor import Tensor\n",
    "from funsor.domains import Array, Bint, Real, Reals\n",
    "from funsor.factory import Bound, Fresh, Has, Value, make_funsor, to_funsor\n",
    "import funsor.ops as ops\n",
    "from funsor.cnf import Contraction\n",
    "from funsor.testing import random_tensor\n",
    "from funsor.interpretations import reflect, memoize\n",
    "import funsor.torch.distributions as dist\n",
    "\n",
    "funsor.set_backend(\"torch\")\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  X^0 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}} \\\\\n",
    "  X^1 &= \\sigma(W^1 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}input}}}{\\vphantom{fg}\\odot}} X^0 + b^1) & W^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1 \\times \\mathsf{\\vphantom{fg}input}} & b^1 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_1} \\\\\n",
    "  X^2 &= \\sigma(W^2 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_1}}{\\vphantom{fg}\\odot}} X^1 + b^2) & W^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2 \\times \\mathsf{\\vphantom{fg}hidden}_1} & b^2 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}_2} \\\\\n",
    "  X^3 &= \\sigma(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}_2}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}\\times \\mathsf{\\vphantom{fg}hidden}_2} & b^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}out}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}[n_0]} \\\\\n",
    "W^l &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer^2}[n_l] \\times \\mathsf{\\vphantom{fg}layer}[n_{l-1}]} \\\\\n",
    "  b^l &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer^2}[n_l]} \\\\\n",
    "  \\text{FullConn}^l(x) &= \\sigma\\left(W^l \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} x + b^l\\right)_{\\mathsf{\\vphantom{fg}layer^2}\\rightarrow\\mathsf{\\vphantom{fg}layer}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def FullConn(\n",
    "    x: Has[{\"layer\"}],\n",
    "    W: Has[{\"layer\"}],\n",
    "    b: Funsor,\n",
    "    layer: Bound\n",
    ") -> Fresh[lambda x: x]:\n",
    "    return ops.sigmoid((W * x).reduce(ops.add, layer) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([9.9856e-01, 9.9938e-01, 1.0000e+00, 9.7161e-01, 1.5346e-02, 9.9998e-01,\n",
       "               1.0000e+00, 3.7765e-01, 8.2557e-01, 5.3595e-05, 9.8989e-01, 2.1241e-01,\n",
       "               4.7784e-04, 9.9399e-01, 9.6523e-07, 9.8700e-01, 1.0000e+00, 9.9633e-01,\n",
       "               1.2932e-04, 5.9441e-03, 9.5259e-01, 1.3116e-06, 5.9127e-03, 1.0000e+00,\n",
       "               1.6307e-01, 6.0132e-06, 9.9991e-01, 7.7603e-01, 9.9606e-01, 9.9999e-01,\n",
       "               1.7991e-07, 6.6572e-05]), {'output': Bint[32]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 100\n",
    "output_size = 32\n",
    "\n",
    "W = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"input\", Bint[input_size]),\n",
    "        (\"output\", Bint[output_size])\n",
    "    ])\n",
    ")\n",
    "b = random_tensor(OrderedDict([(\"output\", Bint[output_size])]))\n",
    "X = random_tensor(OrderedDict([(\"input\", Bint[input_size])]))\n",
    "\n",
    "FullConn(X, W, b, \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x^{t} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}} & t &= 1, \\ldots, n \\\\\n",
    "W^{\\text{h}} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}hidden}^\\prime} & |\\mathsf{\\vphantom{fg}hidden}| &= |\\mathsf{\\vphantom{fg}hidden}^\\prime| \\\\\n",
    "W^{\\text{i}} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}input}\\times \\mathsf{\\vphantom{fg}hidden}^\\prime} \\\\\n",
    "b &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}^\\prime} \\\\\n",
    "h^{0} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}} \\\\\n",
    "h^{t} &= \\sigma\\left( W^{\\text{h}} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}}}{\\vphantom{fg}\\odot}} h^{t-1} + W^{\\text{i}} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}input}}}{\\vphantom{fg}\\odot}} x^{t} + b \\right)_{\\mathsf{\\vphantom{fg}hidden}^\\prime\\rightarrow\\mathsf{\\vphantom{fg}hidden}} & t &= 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def RNN(\n",
    "    x: Has[{\"layer\"}],\n",
    "    Wh: Has[{\"hidden\"}],\n",
    "    Wi: Has[{\"layer\"}],\n",
    "    b: Funsor,\n",
    "    h: Has[{\"hidden\"}],\n",
    "    hidden: Bound,\n",
    "    layer: Bound\n",
    ") -> Fresh[lambda x: x]:\n",
    "    return ops.sigmoid((Wh * h).reduce(ops.add, hidden) + (Wi * x).reduce(ops.add, layer) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([9.3445e-07, 1.0000e+00, 1.0000e+00, 1.8678e-03, 1.2434e-02, 9.9979e-01,\n",
       "               9.9986e-01, 9.8305e-01, 7.2881e-09, 6.8777e-01, 1.6752e-04, 2.2644e-04,\n",
       "               6.9105e-03, 3.6768e-18, 4.9874e-02, 9.2190e-01, 1.0000e+00, 1.0000e+00,\n",
       "               1.0000e+00, 7.9179e-01, 9.7655e-01, 3.0174e-06, 1.1482e-02, 1.0000e+00,\n",
       "               7.9211e-03, 1.1635e-02, 1.0000e+00, 9.7549e-01, 9.8331e-01, 4.5860e-03,\n",
       "               1.0000e+00, 1.8680e-01]), {'hidden2': Bint[32]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 100\n",
    "hidden_size = 32\n",
    "\n",
    "Wh = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden\", Bint[hidden_size]),\n",
    "        (\"hidden2\", Bint[hidden_size])\n",
    "    ])\n",
    ")\n",
    "Wi = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"input\", Bint[input_size]),\n",
    "        (\"hidden2\", Bint[hidden_size])\n",
    "    ])\n",
    ")\n",
    "b = random_tensor(OrderedDict([(\"hidden2\", Bint[hidden_size])]))\n",
    "h = random_tensor(OrderedDict([(\"hidden\", Bint[hidden_size])]))\n",
    "x = random_tensor(OrderedDict([(\"input\", Bint[input_size])]))\n",
    "\n",
    "RNN(x, Wh, Wi, b, h, \"hidden\", \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Softmax(\n",
    "    x: Funsor,\n",
    "    ax: Bound,\n",
    "    ax2: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    x = x(**{ax.name: ax2.name})\n",
    "    y = x - x.reduce(ops.logaddexp, ax2)\n",
    "    return y.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\text{Attention} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}key}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times\\mathsf{\\vphantom{fg}val}} \\times \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}val}} \\\\\n",
    "\\text{Attention}(Q, K, V, M) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\mathrm{softmax}}} \\left( \\frac{Q \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}key}}}{\\vphantom{fg}\\odot}} K}{\\sqrt{|\\mathsf{\\vphantom{fg}key}|}} + M \\right) \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}}}{\\vphantom{fg}\\odot}} V.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Attention(\n",
    "    Q: Has[{\"key\"}],\n",
    "    K: Has[{\"key\", \"seq\"}],\n",
    "    V: Has[{\"seq2\"}],\n",
    "    M: Has[{\"seq\"}],\n",
    "    key: Bound,\n",
    "    seq: Bound,\n",
    "    seq2: Bound\n",
    ") -> Fresh[lambda Q: Q]:\n",
    "    x = (Q * K).reduce(ops.add, key) / math.sqrt(key.output.size) + M\n",
    "    return (Softmax(x, seq, seq2) * V).reduce(ops.add, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([-0.7177, -0.6517, -0.0821,  1.1312,  0.6309]), {'val': Bint[5]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = random_tensor(OrderedDict([(\"key\", Bint[10])]))\n",
    "k = random_tensor(OrderedDict([(\"key\", Bint[10]), (\"seq\", Bint[3])]))\n",
    "v = random_tensor(OrderedDict([(\"seq2\", Bint[3]), (\"val\", Bint[5])]))\n",
    "m = random_tensor(OrderedDict([(\"seq\", Bint[3])]))\n",
    "Attention(q, k, v, m, \"key\", \"seq\", \"seq2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n-|\\mathsf{\\vphantom{fg}kernel}|+1], \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X &= Y,\\ \\text{where} \\\\\n",
    "  Y_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}kernel}(j)} &= X_{\\mathsf{\\vphantom{fg}seq}(i+j - 1)}.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Unroll(\n",
    "    x: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size - k + 1]]\n",
    ") -> Fresh[lambda x: x]:\n",
    "    return x(**{seq.name: seq2 + kernel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\text{Conv1d} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n^\\prime]} \\\\\n",
    "\\text{Conv1d}(X; W, b) &= W \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\odot}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq}\\\\ \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{unroll}}} X + b\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "W &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "b &\\in \\mathbb{R}\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Conv1d(\n",
    "    X: Has[{\"chans\", \"seq\"}],\n",
    "    W: Has[{\"chans\", \"kernel\"}],\n",
    "    b: Funsor,\n",
    "    chans: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Bound,\n",
    "    seq: Bound,\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size - k + 1]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = W * Unroll(X, seq, k, kernel, seq2)\n",
    "    return y.reduce(ops.add, frozenset({chans, kernel})) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([-0.5562, -1.4546,  3.0686, -0.6160,  0.5775,  1.3952, -8.9817,  7.0842]), {'seq2': Bint[8]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"seq\", Bint[10])]))\n",
    "kernel = Variable(\"kernel\", Bint[3])\n",
    "w = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"kernel\", Bint[3])]))\n",
    "b = random_tensor(OrderedDict([]))\n",
    "\n",
    "Conv1d(x, w, b, \"chans\", 3, \"kernel\", \"seq\", \"seq2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{Conv2d} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}height}[h] \\times \\mathsf{\\vphantom{fg}width}[w]}\n",
    "  &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[h2] \\times \\mathsf{\\vphantom{fg}width}[w2]} \\\\\n",
    "  \\text{Conv2d}(X; W, b) &= W \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}\\\\ \\mathsf{\\vphantom{fg}kh}, \\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\odot}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}height}\\\\ \\mathsf{\\vphantom{fg}kh}}}{\\vphantom{fg}\\mathrm{unroll}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}width}\\\\\\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\mathrm{unroll}}} X + b\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}\\times \\mathsf{\\vphantom{fg}kh}\\times \\mathsf{\\vphantom{fg}kw}} \\\\\n",
    "b &\\in \\mathbb{R}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Conv2d(\n",
    "    X: Has[{\"chans\", \"height\", \"width\"}],\n",
    "    W: Has[{\"chans\", \"kh\", \"kw\"}],\n",
    "    b: Funsor,\n",
    "    chans: Bound,\n",
    "    kh_size: Value[int],\n",
    "    kh: Bound,\n",
    "    height: Bound,\n",
    "    height2: Fresh[lambda height, kh_size: Bint[height.size - kh_size + 1]],\n",
    "    kw_size: Value[int],\n",
    "    kw: Bound,\n",
    "    width: Bound,\n",
    "    width2: Fresh[lambda width, kw_size: Bint[width.size - kw_size + 1]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = W * Unroll(Unroll(X, width, kw_size, kw, width2), height, kh_size, kh, height2)\n",
    "    return y.reduce(ops.add, frozenset({chans, kh, kw})) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[ -2.0155,  -7.1308,   3.0204,  -0.1275,  -2.6123],\n",
       "               [ -6.5379,  -0.4569,  -7.0282,   0.9064,   9.3048],\n",
       "               [ -2.5790,   2.8289,   3.5649,  -4.2134,   3.5654],\n",
       "               [ -1.6600,   6.3061,  -1.5722,  -2.4592,  -9.0509],\n",
       "               [  2.7169,   6.0574,  -4.5914,  -0.6295,  -8.4206],\n",
       "               [  4.4485,  11.8542,  -0.5416,  -5.1102,  -2.9582],\n",
       "               [ -1.3208,  -4.6146,   0.9664,  -7.9737,   1.5137],\n",
       "               [  0.3181,   1.1603, -11.0864,   2.2047,  -8.6007]]), {'height2': Bint[8], 'width2': Bint[5]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"height\", Bint[10]), (\"width\", Bint[8])]))\n",
    "w = random_tensor(OrderedDict([(\"chans\", Bint[3]), (\"kh\", Bint[3]), (\"kw\", Bint[4])]))\n",
    "b = random_tensor(OrderedDict([]))\n",
    "\n",
    "Conv2d(x, w, b, \"chans\", 3, \"kh\", \"height\", \"height2\", 4, \"kw\", \"width\", \"width2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq},\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n/|\\mathsf{\\vphantom{fg}kernel}|],\\mathsf{\\vphantom{fg}kernel}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq},\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} X &= Y,\\ \\text{where} \\\\\n",
    "  Y_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}kernel}(j)} &= X_{\\mathsf{\\vphantom{fg}seq}((i-1) \\cdot |\\mathsf{\\vphantom{fg}kernel}| + j)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Pool(\n",
    "    x: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]],\n",
    ") -> Fresh[lambda x: x]:\n",
    "    assert not seq.output.size % k\n",
    "    return x(**{seq.name: seq2 * Number(k, k+1) + kernel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[ 1.3065, -0.2757],\n",
       "               [-0.1250,  1.0614],\n",
       "               [-1.3331, -0.2364],\n",
       "               [ 0.2780, -0.3005],\n",
       "               [ 0.0033, -1.7818]]), {'seq2': Bint[5], 'kernel': Bint[2]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = Pool(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{MaxPool1d}_{k} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}[n/k]} \\\\\n",
    "\\text{MaxPool1d}_{k}(X) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{max}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}seq},\\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} X \\\\\n",
    "|\\mathsf{\\vphantom{fg}kernel}| &= k \\\\\n",
    "\\text{MaxPool2d}_{kh,kw} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[h] \\times \\mathsf{\\vphantom{fg}width}[w]} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}height}[h/kh] \\times \\mathsf{\\vphantom{fg}width}[w/kw]} \\\\\n",
    "\\text{MaxPool2d}_{kh,kw}(X) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}kh},\\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\mathrm{max}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}height},\\mathsf{\\vphantom{fg}kh}}}{\\vphantom{fg}\\mathrm{pool}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}width},\\mathsf{\\vphantom{fg}kw}}}{\\vphantom{fg}\\mathrm{pool}}} X \\\\\n",
    "|\\mathsf{\\vphantom{fg}kh}| &= kh \\\\\n",
    "|\\mathsf{\\vphantom{fg}kw}| &= kw.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool1d(\n",
    "    X: Has[{\"seq\"}],\n",
    "    seq: Bound,\n",
    "    k: Value[int],\n",
    "    kernel: Fresh[lambda k: Bint[k]],\n",
    "    seq2: Fresh[lambda seq, k: Bint[seq.size // k]]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Pool(X, seq, k, kernel, seq2).reduce(ops.max, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([-1.2043,  0.7372,  1.4093,  1.4728,  1.9908]), {'seq2': Bint[5]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"seq\", Bint[10])]))\n",
    "Y = MaxPool1d(X, \"seq\", 2, \"kernel\", \"seq2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def MaxPool2d(\n",
    "    X: Has[{\"height\", \"width\"}],\n",
    "    height: Bound,\n",
    "    kh_size: Value[int],\n",
    "    kh: Fresh[lambda kh_size: Bint[kh_size]],\n",
    "    height2: Fresh[lambda height, kh_size: Bint[height.size // kh_size]],\n",
    "    width: Bound,\n",
    "    kw_size: Value[int],\n",
    "    kw: Fresh[lambda kw_size: Bint[kw_size]],\n",
    "    width2: Fresh[lambda width, kw_size: Bint[width.size // kw_size]],\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = Pool(Pool(X, height, kh_size, kh, height2), width, kw_size, kw, width2)\n",
    "    return y.reduce(ops.max, frozenset({kh, kw}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[0.3430, 0.9254],\n",
       "               [1.1264, 0.9479],\n",
       "               [1.1742, 1.3031]]), {'width2': Bint[3], 'height2': Bint[2]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = random_tensor(OrderedDict([(\"width\", Bint[9]), (\"height\", Bint[4])]))\n",
    "Y = MaxPool2d(X, \"height\", 2, \"kh\", \"height2\", \"width\", 3, \"kw\", \"width2\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{standardize}}} \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}ax}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}ax}} \\\\\n",
    "  \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) &= \\frac{X - \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{mean}}}(X)}{\\sqrt{\\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}ax}}}{\\vphantom{fg}\\mathrm{var}}}(X) + \\epsilon}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Mean(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, ax) / ax.output.size\n",
    "\n",
    "@make_funsor\n",
    "def Mean2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return X.reduce(ops.add, frozenset({ax, ax2})) / (ax.output.size * ax2.output.size)\n",
    "\n",
    "@make_funsor\n",
    "def Variance(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean((X - Mean(X, ax))**2, ax)\n",
    "\n",
    "\n",
    "@make_funsor\n",
    "def Variance2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Mean2((X - Mean2(X, ax, ax2))**2, ax, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Standardize(\n",
    "    X: Has[{\"ax\"}],\n",
    "    ax: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax})\n",
    "    return (y - Mean(X, ax)) / (Variance(X, ax) + ops.finfo(X.data).eps).sqrt()\n",
    "\n",
    "@make_funsor\n",
    "def Standardize2(\n",
    "    X: Has[{\"ax\", \"ax2\"}],\n",
    "    ax: Bound,\n",
    "    ax2: Bound,\n",
    "    new_ax: Fresh[lambda ax: ax],\n",
    "    new_ax2: Fresh[lambda ax2: ax2]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    y = X(**{ax.name: new_ax, ax2.name: new_ax2})\n",
    "    return (y - Mean2(X, ax, ax2)) / (Variance2(X, ax, ax2) + ops.finfo(X.data).eps).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{BatchNorm}(X; \\gamma, \\beta) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}batch},\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta & \\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}} \\\\\n",
    "\\text{InstanceNorm}(X; \\gamma, \\beta) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta & \\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}} \\\\\n",
    "\\text{LayerNorm}(X; \\gamma, \\beta) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer},\\mathsf{\\vphantom{fg}chans}}}{\\vphantom{fg}\\mathrm{standardize}}}(X) \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta & \\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans},\\mathsf{\\vphantom{fg}layer}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def BatchNorm(\n",
    "    X: Has[{\"batch\", \"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    batch: Bound,\n",
    "    layer: Bound,\n",
    "    batch2: Fresh[lambda batch: batch],\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize2(X, batch, layer, batch2, layer2) * gamma + beta\n",
    "\n",
    "@make_funsor\n",
    "def InstanceNorm(\n",
    "    X: Has[{\"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    layer: Bound,\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize(X, layer, layer2) * gamma + beta\n",
    "\n",
    "# same as BatchNorm\n",
    "@make_funsor\n",
    "def LayerNorm(\n",
    "    X: Has[{\"chans\", \"layer\"}],\n",
    "    gamma: Funsor,\n",
    "    beta: Funsor,\n",
    "    chans: Bound,\n",
    "    layer: Bound,\n",
    "    chans2: Fresh[lambda chans: chans],\n",
    "    layer2: Fresh[lambda layer: layer]\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return Standardize2(X, chans, layer, chans2, layer2) * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[ 1.5946e+00,  6.4058e-01, -1.8534e+00,  1.2085e+00,  7.7421e-01],\n",
       "                [-2.1742e+00, -1.2362e+00,  1.5855e+00, -2.1534e+00, -1.6775e+00],\n",
       "                [ 2.7642e-01,  7.3000e-01,  3.5449e-01, -7.2926e-05, -4.5408e-02]],\n",
       "       \n",
       "               [[ 2.3721e+00, -1.3223e+00,  5.5548e-01, -4.9629e-01,  1.7679e+00],\n",
       "                [ 3.3381e+00, -4.5150e+00, -3.5089e+00, -6.4837e-01, -1.7033e+00],\n",
       "                [-1.7505e-02,  6.0665e-01,  2.7615e-01,  8.0836e-01,  6.3741e-01]],\n",
       "       \n",
       "               [[-2.7834e-01, -1.9869e+00, -1.2326e+00,  1.1164e+00,  2.6997e+00],\n",
       "                [-1.5407e+00, -1.0611e+00, -5.3196e-01, -2.1391e+00, -2.7479e+00],\n",
       "                [ 8.3216e-02,  4.7930e-01,  4.0958e-01,  9.8774e-02, -2.5166e-01]],\n",
       "       \n",
       "               [[-2.3987e+00,  3.1820e+00,  5.8014e-01,  1.5299e+00,  1.9554e+00],\n",
       "                [ 1.4750e+00, -3.1413e+00, -3.1395e+00, -1.2250e+00, -2.7671e+00],\n",
       "                [ 1.7551e-01,  3.7150e-01,  6.3553e-01, -3.5381e-01,  3.8984e-01]]]), {'batch2': Bint[4], 'chans': Bint[3], 'layer2': Bint[5]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_tensor(OrderedDict([(\"batch\", Bint[4]), (\"chans\", Bint[3]), (\"layer\", Bint[5])]))\n",
    "g = random_tensor(OrderedDict([(\"chans\", Bint[3])]))\n",
    "b = random_tensor(OrderedDict([(\"chans\", Bint[3])]))\n",
    "\n",
    "BatchNorm(x, g, b, \"batch\", \"layer\", \"batch2\", \"layer2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{GroupNorm}_k(X; \\gamma, \\beta) &= \\left[ \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}kernel},\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{standardize}}} \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}chans}, \\mathsf{\\vphantom{fg}kernel}}}{\\vphantom{fg}\\mathrm{pool}}} X \\right]_{(\\mathsf{\\vphantom{fg}chans},\\mathsf{\\vphantom{fg}kernel})\\rightarrow \\mathsf{\\vphantom{fg}chans}} \\mathbin{\\underset{\\substack{}}{\\vphantom{fg}\\odot}} \\gamma + \\beta \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "|\\mathsf{\\vphantom{fg}kernel}| &= k\\\\\n",
    "\\gamma, \\beta &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  I &\\in \\{0, 1\\}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}vocab}} & \\sum\\limits_{\\substack{\\mathsf{\\vphantom{fg}vocab}}} I &= 1 \\\\\n",
    "  W &= (E \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\odot}} I)\\sqrt{|\\mathsf{\\vphantom{fg}layer}|} & E &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}vocab}\\times \\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  P &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  P_{\\mathsf{\\vphantom{fg}seq}(p), \\mathsf{\\vphantom{fg}layer}(i)} &= \\begin{cases}\n",
    "    \\sin((p-1) / 10000^{(i-1) / |\\mathsf{\\vphantom{fg}layer}|}) & \\text{$i$ odd} \\\\ \n",
    "    \\cos((p-1) / 10000^{(i-2) / |\\mathsf{\\vphantom{fg}layer}|}) & \\text{$i$ even.}\n",
    "  \\end{cases}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "X^0 &= W+P \\\\\n",
    "T^1 &= \\text{LayerNorm}^1(\\text{SelfAtt}^1(X^0)) + X^0\\\\\n",
    "X^1 &= \\text{LayerNorm}^{1^\\prime}(\\text{FFN}^1(T^1)) + T^1\\\\\n",
    "&\\vdotswithin{=} \\\\\n",
    "T^{L} &= \\text{LayerNorm}^L(\\text{SelfAtt}^L(X^{L-1})) + X^{L-1}\\\\\n",
    "X^{L} &= \\text{LayerNorm}^{L^\\prime}(\\text{FFN}^L(T^L)) + T^L\\\\\n",
    "O &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}vocab}}}{\\vphantom{fg}\\mathrm{softmax}}}(E \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X^L)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{LayerNorm}^l \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  \\text{LayerNorm}^l(X) &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\mathrm{XNorm}}}(X; \\beta^l, \\gamma^l).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{SelfAtt}^l \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}layer}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  \\text{SelfAtt}^l(X) &= Y\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  |\\mathsf{\\vphantom{fg}seq}| &= |\\mathsf{\\vphantom{fg}seq2}| \\\\\n",
    "  |\\mathsf{\\vphantom{fg}key}| = |\\mathsf{\\vphantom{fg}val}| &= |\\mathsf{\\vphantom{fg}layer}|/|\\mathsf{\\vphantom{fg}heads}| \\\\\n",
    "  Q &= W^{l,Q} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X_{\\mathsf{\\vphantom{fg}seq}\\rightarrow\\mathsf{\\vphantom{fg}seq2}} & W^{l,Q} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}key}} \\\\\n",
    "  K &= W^{l,K} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X & W^{l,K} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}key}} \\\\\n",
    "  V &= W^{l,V} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X & W^{l,V} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}val}} \\\\\n",
    "  M & \\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}seq}\\times \\mathsf{\\vphantom{fg}seq2}} \\\\\n",
    "  M_{\\mathsf{\\vphantom{fg}seq}(i), \\mathsf{\\vphantom{fg}seq2}(j)} &= \\begin{cases}\n",
    "    0 & i \\leq j\\\\\n",
    "    -\\infty & \\text{otherwise}\n",
    "  \\end{cases} \\\\\n",
    "  Y &= W^{l,O} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}heads}\\\\ \\mathsf{\\vphantom{fg}val}}}{\\vphantom{fg}\\odot}} \\text{Attention}(Q, K, V, M)_{\\mathsf{\\vphantom{fg}seq2}\\rightarrow\\mathsf{\\vphantom{fg}seq}} & W^{l,O} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}heads}\\times \\mathsf{\\vphantom{fg}val}\\times \\mathsf{\\vphantom{fg}layer}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  \\text{FFN}^l \\colon \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} &\\rightarrow \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "  \\text{FFN}^l(X) &= X^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "  X^1 &= \\text{relu}(W^{l,1} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X + b^{l,1}) & W^{l,1} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}layer}} & b^{l,1} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}} \\\\\n",
    "  X^2 &= \\text{relu}(W^{l,2} \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}}}{\\vphantom{fg}\\odot}} X^1 + b^{l,2}) & W^{l,2} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}layer}\\times \\mathsf{\\vphantom{fg}hidden}} & b^{l,2} &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "X^0 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}batch}\\times \\mathsf{\\vphantom{fg}chans}[c_0] \\times \\mathsf{\\vphantom{fg}height}\\times \\mathsf{\\vphantom{fg}width}} \\\\\n",
    "T^1 &= \\text{relu}(\\text{Conv}^1(X^0)) \\\\\n",
    "X^1 &= \\text{MaxPool}^1(T^1) \\\\\n",
    "T^2 &= \\text{relu}(\\text{Conv}^2(X^1)) \\\\\n",
    "X^2 &= \\text{MaxPool}^2(T^2)_{(\\mathsf{\\vphantom{fg}height},\\mathsf{\\vphantom{fg}width},\\mathsf{\\vphantom{fg}chans})\\rightarrow\\mathsf{\\vphantom{fg}layer}} \\\\\n",
    "X^3 &= \\text{relu}(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}layer}}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}layer}} & b^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}} \\\\\n",
    "O &= \\mathop{\\underset{\\substack{\\mathsf{\\vphantom{fg}classes}}}{\\vphantom{fg}\\mathrm{softmax}}} (W^4 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}hidden}}}{\\vphantom{fg}\\odot}} X^3 + b^4) & W^4 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}classes}\\times \\mathsf{\\vphantom{fg}hidden}} & b^4 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}classes}}\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "X^2 &= \\text{MaxPool}^2(T^2) \\\\\n",
    "X^3 &= \\text{relu}(W^3 \\mathbin{\\underset{\\substack{\\mathsf{\\vphantom{fg}height}\\\\ \\mathsf{\\vphantom{fg}width}\\\\ \\mathsf{\\vphantom{fg}chans}}}{\\vphantom{fg}\\odot}} X^2 + b^3) & W^3 &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}hidden}\\times \\mathsf{\\vphantom{fg}height}\\times \\mathsf{\\vphantom{fg}width}\\times \\mathsf{\\vphantom{fg}chans}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Conv}^l(X) &= \\text{Conv2d}(X; W^l, b^l)_{\\mathsf{\\vphantom{fg}chans2}\\rightarrow\\mathsf{\\vphantom{fg}chans}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W^l & \\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans2}[c_l] \\times \\mathsf{\\vphantom{fg}chans}[c_{l-1}] \\times \\mathsf{\\vphantom{fg}kh}[kh_l] \\times \\mathsf{\\vphantom{fg}kw}[kw_l]} \\\\\n",
    "b^l &\\in \\mathbb{R}^{\\mathsf{\\vphantom{fg}chans2}[c_l]}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{MaxPool}^l(X) &amp;= \\text{MaxPool2d}_{ph^l,ph^l}(X).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@make_funsor\n",
    "def Relu(\n",
    "    X: Funsor\n",
    ") -> Fresh[lambda X: X]:\n",
    "    return ops.max(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(tensor([[[0.0000, 0.0000, 0.6447, 0.0000, 0.0000],\n",
       "                [0.3316, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "                [0.0236, 0.4104, 0.0000, 0.0499, 1.2986]],\n",
       "       \n",
       "               [[0.0000, 1.6719, 0.3126, 0.1956, 0.0000],\n",
       "                [0.0399, 0.3172, 0.0000, 0.0000, 0.0000],\n",
       "                [0.0000, 0.0000, 0.0000, 0.5740, 0.0000]],\n",
       "       \n",
       "               [[0.0000, 1.0173, 0.0000, 0.0000, 0.4141],\n",
       "                [1.2899, 0.0000, 0.0000, 0.0000, 0.8552],\n",
       "                [1.5997, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "       \n",
       "               [[0.0000, 0.0678, 0.1885, 0.0000, 1.5414],\n",
       "                [0.0000, 0.0641, 0.0000, 0.4651, 0.0000],\n",
       "                [0.0000, 0.0000, 1.0786, 0.0000, 0.0000]]]), {'batch': Bint[4], 'chans': Bint[3], 'layer': Bint[5]})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"chans\", Bint[3]),\n",
    "        (\"kh\", Bint[3]),\n",
    "        (\"kw\", Bint[4]),\n",
    "        (\"chans2\", Bint[3])\n",
    "    ]),\n",
    ")\n",
    "b1 = random_tensor(OrderedDict([(\"chans2\", Bint[3])]))\n",
    "W3 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden\", Bint[3]),\n",
    "        (\"height3\", Bint[4]),\n",
    "        (\"width3\", Bint[4]),\n",
    "        (\"chans2\", Bint[3])\n",
    "    ]),\n",
    ")\n",
    "b3 = random_tensor(OrderedDict([(\"hidden\", Bint[3])]))\n",
    "W4 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"hidden\", Bint[3]),\n",
    "        (\"classes\", Bint[5]),\n",
    "    ]),\n",
    ")\n",
    "b4 = random_tensor(OrderedDict([(\"classes\", Bint[5])]))\n",
    "\n",
    "X0 = random_tensor(\n",
    "    OrderedDict([\n",
    "        (\"batch\", Bint[4]),\n",
    "        (\"chans\", Bint[3]),\n",
    "        (\"height\", Bint[14]),\n",
    "        (\"width\", Bint[15])\n",
    "    ])\n",
    ")\n",
    "\n",
    "T1 = Relu(\n",
    "    Conv2d(X0, W1, b1, \"chans\", 3, \"kh\", \"height\", \"height2\", 4, \"kw\", \"width\", \"width2\")\n",
    ")\n",
    "X1 = MaxPool2d(T1, \"height2\", 3, \"kh\", \"height3\", \"width2\", 3, \"kw\", \"width3\")\n",
    "X3 = (W3 * X1).reduce(ops.add, frozenset({\"height3\", \"width3\", \"chans2\"})) + b3\n",
    "O = Softmax(((W4 * X3).reduce(ops.add, \"hidden\") + b4), \"classes\", \"classes2\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
